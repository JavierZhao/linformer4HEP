{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input key: input_1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model.\n",
    "model_dir = '/Users/anrunw/Downloads/l1-jet-id/scripts/trained_mlps/mlp_8bit_8const/kfolding1/'\n",
    "loaded_model = tf.saved_model.load(model_dir)\n",
    "loaded_model.trainable = False\n",
    "\n",
    "# Use the default serving signature.\n",
    "infer = loaded_model.signatures[\"serving_default\"]\n",
    "\n",
    "# Determine the input key from the signature.\n",
    "input_key = list(infer.structured_input_signature[1].keys())[0]\n",
    "print(\"Using input key:\", input_key)\n",
    "\n",
    "# Wrap the serving signature in a Keras model.\n",
    "class LoadedModelWrapper(tf.keras.Model):\n",
    "    def __init__(self, infer, input_key):\n",
    "        super(LoadedModelWrapper, self).__init__()\n",
    "        self.infer = infer\n",
    "        self.input_key = input_key\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Call the serving signature using the correct input key.\n",
    "        outputs = self.infer(**{self.input_key: inputs})\n",
    "        # If the signature returns a dict, extract the first output.\n",
    "        # (Adjust this if you know your output key.)\n",
    "        return list(outputs.values())[0]\n",
    "\n",
    "# Create an instance of the wrapper.\n",
    "model = LoadedModelWrapper(infer, input_key)\n",
    "\n",
    "# Now you can run inference like a normal TF model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class AggregationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Aggregates a set of features over the sequence dimension.\n",
    "    Supported aggregations: \"mean\" or \"max\".\n",
    "    \"\"\"\n",
    "    def __init__(self, aggreg=\"mean\", **kwargs):\n",
    "        super(AggregationLayer, self).__init__(**kwargs)\n",
    "        self.aggreg = aggreg\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.aggreg == \"mean\":\n",
    "            return tf.reduce_mean(inputs, axis=1)\n",
    "        elif self.aggreg == \"max\":\n",
    "            return tf.reduce_max(inputs, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Given aggregation string is not implemented. Use 'mean' or 'max'.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Gumbel Softmax Layer with Hard Sampling\n",
    "# ---------------------------\n",
    "# Custom Layers\n",
    "# ---------------------------\n",
    "class CustomMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super(CustomMultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len, d_model)\n",
    "        self.wq = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wq\")\n",
    "        self.wk = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wk\")\n",
    "        self.wv = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wv\")\n",
    "        self.dense = layers.Dense(self.d_model)\n",
    "        super(CustomMultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        # (batch_size, num_heads, seq_len, depth)\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Linear projections for queries, keys, and values.\n",
    "        q = tf.matmul(x, self.wq)  # (batch_size, seq_len, d_model)\n",
    "        k = tf.matmul(x, self.wk)\n",
    "        v = tf.matmul(x, self.wv)\n",
    "\n",
    "        # Split into heads.\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled dot-product attention.\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        scaled_attention = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Concatenate heads.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Final linear layer.\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, output_dim, num_heads, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.mha = CustomMultiHeadAttention(d_model, num_heads)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.outD = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Multi-head attention sub-layer.\n",
    "        attn_output = self.mha(x)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        # Feed-forward sub-layer.\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        out2 = self.outD(out2)\n",
    "        return out2\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definition\n",
    "# ---------------------------\n",
    "def build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                          d_model=16, d_ff=16, output_dim=16,\n",
    "                                          num_heads=1, attn_temperature=1.0):\n",
    "    \"\"\"\n",
    "    Builds a classifier model with:\n",
    "      - A linear embedding layer.\n",
    "      - Multiple transformer encoder blocks using Gumbel Softmax in attention.\n",
    "      - Aggregation over the sequence dimension using AggregationLayer.\n",
    "      - A final linear output layer for 5 classes.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_particles, feature_dim))\n",
    "    \n",
    "    # Linear embedding.\n",
    "    x = layers.Dense(16, activation='relu')(inputs)\n",
    "\n",
    "    # Apply multiple transformer blocks.\n",
    "    x = TransformerBlock(16, 16, output_dim=16, num_heads=8)(x)\n",
    "    x = TransformerBlock(16, 16, output_dim=16, num_heads=8)(x)\n",
    "    \n",
    "    # Custom aggregation (using \"max\" aggregation here).\n",
    "    pooled_output = AggregationLayer(aggreg='max')(x)\n",
    "    \n",
    "    # Final dense layers.\n",
    "    x = layers.Dense(16, activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# ---------------------------\n",
    "# Example Training Loop\n",
    "# ---------------------------\n",
    "import numpy as np\n",
    "\n",
    "# Load your data. Adjust file paths if needed.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the original training data\n",
    "x_train = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/x_train_robust_32const_ptetaphi.npy\")\n",
    "y_train = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/y_train_robust_32const_ptetaphi.npy\")\n",
    "\n",
    "# Perform an 80/20 split on the original training data\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Determine input dimensions from data.\n",
    "num_particles = x_train.shape[1]\n",
    "feature_dim = x_train.shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "# Print model summary.\n",
    "\n",
    "# Train the model.\n",
    "\n",
    "\n",
    "# Optionally, print training history.\n",
    "#print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/x_train_robust_32const_ptetaphi.npy\")\n",
    "y_train = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/y_train_robust_32const_ptetaphi.npy\")\n",
    "\n",
    "# Perform an 80/20 split on the original training data\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m1,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m1,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,261</span> (16.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,261\u001b[0m (16.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,261</span> (16.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,261\u001b[0m (16.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                            d_model=8, d_ff=8,\n",
    "                                            output_dim=8, num_heads=4,\n",
    "                                            )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0003),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 50ms/step - accuracy: 0.5455 - loss: 1.1600 - val_accuracy: 0.6474 - val_loss: 0.9530\n",
      "Epoch 2/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.6586 - loss: 0.9288 - val_accuracy: 0.6819 - val_loss: 0.8787\n",
      "Epoch 3/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.6897 - loss: 0.8638 - val_accuracy: 0.6996 - val_loss: 0.8319\n",
      "Epoch 4/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7135 - loss: 0.8089 - val_accuracy: 0.7252 - val_loss: 0.7822\n",
      "Epoch 5/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7283 - loss: 0.7738 - val_accuracy: 0.7336 - val_loss: 0.7568\n",
      "Epoch 6/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7367 - loss: 0.7496 - val_accuracy: 0.7338 - val_loss: 0.7550\n",
      "Epoch 7/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7408 - loss: 0.7364 - val_accuracy: 0.7443 - val_loss: 0.7267\n",
      "Epoch 8/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7450 - loss: 0.7247 - val_accuracy: 0.7470 - val_loss: 0.7178\n",
      "Epoch 9/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7496 - loss: 0.7120 - val_accuracy: 0.7503 - val_loss: 0.7090\n",
      "Epoch 10/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7520 - loss: 0.7071 - val_accuracy: 0.7532 - val_loss: 0.7021\n",
      "Epoch 11/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7545 - loss: 0.6998 - val_accuracy: 0.7569 - val_loss: 0.6942\n",
      "Epoch 12/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7578 - loss: 0.6923 - val_accuracy: 0.7590 - val_loss: 0.6873\n",
      "Epoch 13/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7586 - loss: 0.6875 - val_accuracy: 0.7611 - val_loss: 0.6824\n",
      "Epoch 14/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7623 - loss: 0.6796 - val_accuracy: 0.7624 - val_loss: 0.6794\n",
      "Epoch 15/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 51ms/step - accuracy: 0.7634 - loss: 0.6754 - val_accuracy: 0.7626 - val_loss: 0.6774\n",
      "Epoch 16/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7647 - loss: 0.6724 - val_accuracy: 0.7613 - val_loss: 0.6806\n",
      "Epoch 17/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7656 - loss: 0.6715 - val_accuracy: 0.7653 - val_loss: 0.6689\n",
      "Epoch 18/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7658 - loss: 0.6684 - val_accuracy: 0.7655 - val_loss: 0.6712\n",
      "Epoch 19/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7667 - loss: 0.6660 - val_accuracy: 0.7658 - val_loss: 0.6702\n",
      "Epoch 20/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 51ms/step - accuracy: 0.7688 - loss: 0.6619 - val_accuracy: 0.7676 - val_loss: 0.6634\n",
      "Epoch 21/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7691 - loss: 0.6605 - val_accuracy: 0.7697 - val_loss: 0.6586\n",
      "Epoch 22/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7693 - loss: 0.6593 - val_accuracy: 0.7672 - val_loss: 0.6671\n",
      "Epoch 23/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7694 - loss: 0.6584 - val_accuracy: 0.7706 - val_loss: 0.6559\n",
      "Epoch 24/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7718 - loss: 0.6539 - val_accuracy: 0.7719 - val_loss: 0.6526\n",
      "Epoch 25/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7731 - loss: 0.6508 - val_accuracy: 0.7730 - val_loss: 0.6506\n",
      "Epoch 26/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7727 - loss: 0.6510 - val_accuracy: 0.7709 - val_loss: 0.6538\n",
      "Epoch 27/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7742 - loss: 0.6476 - val_accuracy: 0.7724 - val_loss: 0.6502\n",
      "Epoch 28/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7735 - loss: 0.6484 - val_accuracy: 0.7732 - val_loss: 0.6483\n",
      "Epoch 29/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7739 - loss: 0.6465 - val_accuracy: 0.7713 - val_loss: 0.6537\n",
      "Epoch 30/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7737 - loss: 0.6479 - val_accuracy: 0.7757 - val_loss: 0.6451\n",
      "Epoch 31/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7756 - loss: 0.6440 - val_accuracy: 0.7757 - val_loss: 0.6436\n",
      "Epoch 32/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7768 - loss: 0.6405 - val_accuracy: 0.7761 - val_loss: 0.6425\n",
      "Epoch 33/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7763 - loss: 0.6408 - val_accuracy: 0.7763 - val_loss: 0.6429\n",
      "Epoch 34/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7765 - loss: 0.6402 - val_accuracy: 0.7752 - val_loss: 0.6450\n",
      "Epoch 35/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7779 - loss: 0.6364 - val_accuracy: 0.7774 - val_loss: 0.6396\n",
      "Epoch 36/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7761 - loss: 0.6399 - val_accuracy: 0.7770 - val_loss: 0.6411\n",
      "Epoch 37/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7773 - loss: 0.6385 - val_accuracy: 0.7756 - val_loss: 0.6428\n",
      "Epoch 38/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7783 - loss: 0.6357 - val_accuracy: 0.7783 - val_loss: 0.6363\n",
      "Epoch 39/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7774 - loss: 0.6366 - val_accuracy: 0.7762 - val_loss: 0.6415\n",
      "Epoch 40/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 51ms/step - accuracy: 0.7771 - loss: 0.6378 - val_accuracy: 0.7785 - val_loss: 0.6352\n",
      "Epoch 41/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7780 - loss: 0.6346 - val_accuracy: 0.7780 - val_loss: 0.6377\n",
      "Epoch 42/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7788 - loss: 0.6330 - val_accuracy: 0.7784 - val_loss: 0.6367\n",
      "Epoch 43/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7777 - loss: 0.6337 - val_accuracy: 0.7763 - val_loss: 0.6417\n",
      "Epoch 44/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7792 - loss: 0.6331 - val_accuracy: 0.7798 - val_loss: 0.6324\n",
      "Epoch 45/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.7782 - loss: 0.6339 - val_accuracy: 0.7799 - val_loss: 0.6329\n",
      "Epoch 46/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7806 - loss: 0.6277 - val_accuracy: 0.7798 - val_loss: 0.6328\n",
      "Epoch 47/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7801 - loss: 0.6292 - val_accuracy: 0.7804 - val_loss: 0.6312\n",
      "Epoch 48/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.7812 - loss: 0.6285 - val_accuracy: 0.7809 - val_loss: 0.6303\n",
      "Epoch 49/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7807 - loss: 0.6267 - val_accuracy: 0.7805 - val_loss: 0.6301\n",
      "Epoch 50/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7808 - loss: 0.6283 - val_accuracy: 0.7803 - val_loss: 0.6303\n",
      "Epoch 51/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 53ms/step - accuracy: 0.7809 - loss: 0.6270 - val_accuracy: 0.7788 - val_loss: 0.6333\n",
      "Epoch 52/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7811 - loss: 0.6261 - val_accuracy: 0.7813 - val_loss: 0.6285\n",
      "Epoch 53/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7818 - loss: 0.6246 - val_accuracy: 0.7806 - val_loss: 0.6298\n",
      "Epoch 54/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7806 - loss: 0.6269 - val_accuracy: 0.7814 - val_loss: 0.6286\n",
      "Epoch 55/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7820 - loss: 0.6240 - val_accuracy: 0.7821 - val_loss: 0.6264\n",
      "Epoch 56/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7817 - loss: 0.6253 - val_accuracy: 0.7784 - val_loss: 0.6349\n",
      "Epoch 57/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.7806 - loss: 0.6264 - val_accuracy: 0.7818 - val_loss: 0.6253\n",
      "Epoch 58/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7819 - loss: 0.6235 - val_accuracy: 0.7810 - val_loss: 0.6276\n",
      "Epoch 59/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.7818 - loss: 0.6248 - val_accuracy: 0.7820 - val_loss: 0.6256\n",
      "Epoch 60/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7822 - loss: 0.6244 - val_accuracy: 0.7811 - val_loss: 0.6274\n",
      "Epoch 61/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7816 - loss: 0.6252 - val_accuracy: 0.7804 - val_loss: 0.6299\n",
      "Epoch 62/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7814 - loss: 0.6230 - val_accuracy: 0.7794 - val_loss: 0.6327\n",
      "Epoch 63/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7820 - loss: 0.6229 - val_accuracy: 0.7802 - val_loss: 0.6287\n",
      "Epoch 64/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7828 - loss: 0.6211 - val_accuracy: 0.7808 - val_loss: 0.6278\n",
      "Epoch 65/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.7815 - loss: 0.6236 - val_accuracy: 0.7808 - val_loss: 0.6277\n",
      "Epoch 66/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.7819 - loss: 0.6223 - val_accuracy: 0.7817 - val_loss: 0.6266\n",
      "Epoch 67/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7830 - loss: 0.6195 - val_accuracy: 0.7809 - val_loss: 0.6259\n",
      "Epoch 68/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.7824 - loss: 0.6214 - val_accuracy: 0.7812 - val_loss: 0.6260\n",
      "Epoch 69/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.7830 - loss: 0.6212 - val_accuracy: 0.7826 - val_loss: 0.6226\n",
      "Epoch 70/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7838 - loss: 0.6173 - val_accuracy: 0.7805 - val_loss: 0.6279\n",
      "Epoch 71/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7839 - loss: 0.6187 - val_accuracy: 0.7810 - val_loss: 0.6269\n",
      "Epoch 72/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7834 - loss: 0.6188 - val_accuracy: 0.7787 - val_loss: 0.6331\n",
      "Epoch 73/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7834 - loss: 0.6199 - val_accuracy: 0.7823 - val_loss: 0.6226\n",
      "Epoch 74/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7825 - loss: 0.6200 - val_accuracy: 0.7834 - val_loss: 0.6200\n",
      "Epoch 75/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7833 - loss: 0.6187 - val_accuracy: 0.7814 - val_loss: 0.6245\n",
      "Epoch 76/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7839 - loss: 0.6173 - val_accuracy: 0.7826 - val_loss: 0.6214\n",
      "Epoch 77/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.7835 - loss: 0.6175 - val_accuracy: 0.7822 - val_loss: 0.6234\n",
      "Epoch 78/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.7840 - loss: 0.6180 - val_accuracy: 0.7838 - val_loss: 0.6196\n",
      "Epoch 79/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 51ms/step - accuracy: 0.7836 - loss: 0.6179 - val_accuracy: 0.7800 - val_loss: 0.6279\n",
      "Epoch 80/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 53ms/step - accuracy: 0.7825 - loss: 0.6203 - val_accuracy: 0.7828 - val_loss: 0.6207\n",
      "Epoch 81/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7851 - loss: 0.6151 - val_accuracy: 0.7833 - val_loss: 0.6205\n",
      "Epoch 82/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7846 - loss: 0.6151 - val_accuracy: 0.7828 - val_loss: 0.6197\n",
      "Epoch 83/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7863 - loss: 0.6134 - val_accuracy: 0.7831 - val_loss: 0.6211\n",
      "Epoch 84/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.7844 - loss: 0.6155 - val_accuracy: 0.7837 - val_loss: 0.6184\n",
      "Epoch 85/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7843 - loss: 0.6157 - val_accuracy: 0.7834 - val_loss: 0.6196\n",
      "Epoch 86/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7833 - loss: 0.6190 - val_accuracy: 0.7827 - val_loss: 0.6204\n",
      "Epoch 87/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7843 - loss: 0.6150 - val_accuracy: 0.7837 - val_loss: 0.6181\n",
      "Epoch 88/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7842 - loss: 0.6152 - val_accuracy: 0.7821 - val_loss: 0.6214\n",
      "Epoch 89/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7847 - loss: 0.6155 - val_accuracy: 0.7832 - val_loss: 0.6193\n",
      "Epoch 90/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7845 - loss: 0.6153 - val_accuracy: 0.7828 - val_loss: 0.6205\n",
      "Epoch 91/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7842 - loss: 0.6151 - val_accuracy: 0.7831 - val_loss: 0.6191\n",
      "Epoch 92/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7851 - loss: 0.6132 - val_accuracy: 0.7839 - val_loss: 0.6172\n",
      "Epoch 93/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7850 - loss: 0.6148 - val_accuracy: 0.7817 - val_loss: 0.6239\n",
      "Epoch 94/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7839 - loss: 0.6144 - val_accuracy: 0.7826 - val_loss: 0.6218\n",
      "Epoch 95/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.7843 - loss: 0.6161 - val_accuracy: 0.7841 - val_loss: 0.6162\n",
      "Epoch 96/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7853 - loss: 0.6124 - val_accuracy: 0.7838 - val_loss: 0.6172\n",
      "Epoch 97/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7842 - loss: 0.6152 - val_accuracy: 0.7837 - val_loss: 0.6162\n",
      "Epoch 98/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7862 - loss: 0.6102 - val_accuracy: 0.7837 - val_loss: 0.6181\n",
      "Epoch 99/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7868 - loss: 0.6119 - val_accuracy: 0.7836 - val_loss: 0.6187\n",
      "Epoch 100/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7858 - loss: 0.6119 - val_accuracy: 0.7850 - val_loss: 0.6150\n",
      "Epoch 101/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7843 - loss: 0.6153 - val_accuracy: 0.7840 - val_loss: 0.6165\n",
      "Epoch 102/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7864 - loss: 0.6095 - val_accuracy: 0.7814 - val_loss: 0.6238\n",
      "Epoch 103/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7856 - loss: 0.6115 - val_accuracy: 0.7832 - val_loss: 0.6198\n",
      "Epoch 104/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7857 - loss: 0.6116 - val_accuracy: 0.7824 - val_loss: 0.6211\n",
      "Epoch 105/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7865 - loss: 0.6103 - val_accuracy: 0.7793 - val_loss: 0.6296\n",
      "Epoch 106/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7864 - loss: 0.6105 - val_accuracy: 0.7821 - val_loss: 0.6218\n",
      "Epoch 107/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7854 - loss: 0.6124 - val_accuracy: 0.7844 - val_loss: 0.6151\n",
      "Epoch 108/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7864 - loss: 0.6091 - val_accuracy: 0.7847 - val_loss: 0.6153\n",
      "Epoch 109/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7862 - loss: 0.6105 - val_accuracy: 0.7840 - val_loss: 0.6176\n",
      "Epoch 110/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7860 - loss: 0.6103 - val_accuracy: 0.7836 - val_loss: 0.6172\n",
      "Epoch 111/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.7864 - loss: 0.6098 - val_accuracy: 0.7847 - val_loss: 0.6146\n",
      "Epoch 112/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7858 - loss: 0.6125 - val_accuracy: 0.7852 - val_loss: 0.6138\n",
      "Epoch 113/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7856 - loss: 0.6099 - val_accuracy: 0.7844 - val_loss: 0.6170\n",
      "Epoch 114/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7869 - loss: 0.6096 - val_accuracy: 0.7851 - val_loss: 0.6134\n",
      "Epoch 115/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.7864 - loss: 0.6086 - val_accuracy: 0.7843 - val_loss: 0.6167\n",
      "Epoch 116/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7871 - loss: 0.6080 - val_accuracy: 0.7841 - val_loss: 0.6162\n",
      "Epoch 117/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7869 - loss: 0.6081 - val_accuracy: 0.7841 - val_loss: 0.6174\n",
      "Epoch 118/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7872 - loss: 0.6085 - val_accuracy: 0.7846 - val_loss: 0.6150\n",
      "Epoch 119/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7867 - loss: 0.6091 - val_accuracy: 0.7849 - val_loss: 0.6144\n",
      "Epoch 120/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7870 - loss: 0.6090 - val_accuracy: 0.7854 - val_loss: 0.6137\n",
      "Epoch 121/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7861 - loss: 0.6093 - val_accuracy: 0.7850 - val_loss: 0.6131\n",
      "Epoch 122/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7872 - loss: 0.6070 - val_accuracy: 0.7847 - val_loss: 0.6147\n",
      "Epoch 123/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7869 - loss: 0.6075 - val_accuracy: 0.7855 - val_loss: 0.6128\n",
      "Epoch 124/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7872 - loss: 0.6075 - val_accuracy: 0.7861 - val_loss: 0.6112\n",
      "Epoch 125/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7888 - loss: 0.6053 - val_accuracy: 0.7832 - val_loss: 0.6167\n",
      "Epoch 126/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7882 - loss: 0.6057 - val_accuracy: 0.7847 - val_loss: 0.6147\n",
      "Epoch 127/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7871 - loss: 0.6067 - val_accuracy: 0.7856 - val_loss: 0.6120\n",
      "Epoch 128/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 53ms/step - accuracy: 0.7859 - loss: 0.6083 - val_accuracy: 0.7858 - val_loss: 0.6131\n",
      "Epoch 129/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7873 - loss: 0.6066 - val_accuracy: 0.7860 - val_loss: 0.6117\n",
      "Epoch 130/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7877 - loss: 0.6050 - val_accuracy: 0.7853 - val_loss: 0.6122\n",
      "Epoch 131/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7870 - loss: 0.6068 - val_accuracy: 0.7838 - val_loss: 0.6162\n",
      "Epoch 132/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7878 - loss: 0.6049 - val_accuracy: 0.7857 - val_loss: 0.6121\n",
      "Epoch 133/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7868 - loss: 0.6074 - val_accuracy: 0.7848 - val_loss: 0.6140\n",
      "Epoch 134/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7867 - loss: 0.6090 - val_accuracy: 0.7858 - val_loss: 0.6119\n",
      "Epoch 135/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7883 - loss: 0.6052 - val_accuracy: 0.7838 - val_loss: 0.6161\n",
      "Epoch 136/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7874 - loss: 0.6062 - val_accuracy: 0.7849 - val_loss: 0.6136\n",
      "Epoch 137/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7873 - loss: 0.6057 - val_accuracy: 0.7854 - val_loss: 0.6140\n",
      "Epoch 138/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7883 - loss: 0.6032 - val_accuracy: 0.7860 - val_loss: 0.6114\n",
      "Epoch 139/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 61ms/step - accuracy: 0.7889 - loss: 0.6018 - val_accuracy: 0.7854 - val_loss: 0.6126\n",
      "Epoch 140/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 61ms/step - accuracy: 0.7881 - loss: 0.6050 - val_accuracy: 0.7854 - val_loss: 0.6116\n",
      "Epoch 141/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 1s/step - accuracy: 0.7873 - loss: 0.6070 - val_accuracy: 0.7858 - val_loss: 0.6118\n",
      "Epoch 142/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 72ms/step - accuracy: 0.7874 - loss: 0.6049 - val_accuracy: 0.7862 - val_loss: 0.6107\n",
      "Epoch 143/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7878 - loss: 0.6041 - val_accuracy: 0.7824 - val_loss: 0.6193\n",
      "Epoch 144/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 139ms/step - accuracy: 0.7876 - loss: 0.6062 - val_accuracy: 0.7808 - val_loss: 0.6242\n",
      "Epoch 145/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7892 - loss: 0.6025 - val_accuracy: 0.7838 - val_loss: 0.6150\n",
      "Epoch 146/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 69ms/step - accuracy: 0.7891 - loss: 0.6027 - val_accuracy: 0.7862 - val_loss: 0.6109\n",
      "Epoch 147/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7871 - loss: 0.6044 - val_accuracy: 0.7840 - val_loss: 0.6160\n",
      "Epoch 148/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 193ms/step - accuracy: 0.7884 - loss: 0.6043 - val_accuracy: 0.7852 - val_loss: 0.6132\n",
      "Epoch 149/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7886 - loss: 0.6037 - val_accuracy: 0.7861 - val_loss: 0.6099\n",
      "Epoch 150/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7869 - loss: 0.6058 - val_accuracy: 0.7845 - val_loss: 0.6156\n",
      "Epoch 151/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7872 - loss: 0.6063 - val_accuracy: 0.7851 - val_loss: 0.6118\n",
      "Epoch 152/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.7885 - loss: 0.6042 - val_accuracy: 0.7862 - val_loss: 0.6111\n",
      "Epoch 153/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.7880 - loss: 0.6052 - val_accuracy: 0.7858 - val_loss: 0.6117\n",
      "Epoch 154/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7890 - loss: 0.6024 - val_accuracy: 0.7865 - val_loss: 0.6103\n",
      "Epoch 155/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.7893 - loss: 0.6013 - val_accuracy: 0.7852 - val_loss: 0.6127\n",
      "Epoch 156/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7887 - loss: 0.6010 - val_accuracy: 0.7861 - val_loss: 0.6099\n",
      "Epoch 157/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7867 - loss: 0.6062 - val_accuracy: 0.7860 - val_loss: 0.6127\n",
      "Epoch 158/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7889 - loss: 0.6036 - val_accuracy: 0.7863 - val_loss: 0.6095\n",
      "Epoch 159/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7906 - loss: 0.6000 - val_accuracy: 0.7838 - val_loss: 0.6160\n",
      "Epoch 160/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 72ms/step - accuracy: 0.7885 - loss: 0.6028 - val_accuracy: 0.7859 - val_loss: 0.6127\n",
      "Epoch 161/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 90ms/step - accuracy: 0.7886 - loss: 0.6032 - val_accuracy: 0.7862 - val_loss: 0.6101\n",
      "Epoch 162/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 71ms/step - accuracy: 0.7888 - loss: 0.6004 - val_accuracy: 0.7873 - val_loss: 0.6085\n",
      "Epoch 163/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 65ms/step - accuracy: 0.7877 - loss: 0.6038 - val_accuracy: 0.7857 - val_loss: 0.6108\n",
      "Epoch 164/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7885 - loss: 0.6039 - val_accuracy: 0.7830 - val_loss: 0.6203\n",
      "Epoch 165/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7883 - loss: 0.6038 - val_accuracy: 0.7870 - val_loss: 0.6085\n",
      "Epoch 166/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7891 - loss: 0.6010 - val_accuracy: 0.7849 - val_loss: 0.6132\n",
      "Epoch 167/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7880 - loss: 0.6045 - val_accuracy: 0.7866 - val_loss: 0.6087\n",
      "Epoch 168/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 75ms/step - accuracy: 0.7893 - loss: 0.6016 - val_accuracy: 0.7805 - val_loss: 0.6257\n",
      "Epoch 169/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 72ms/step - accuracy: 0.7878 - loss: 0.6029 - val_accuracy: 0.7855 - val_loss: 0.6114\n",
      "Epoch 170/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 70ms/step - accuracy: 0.7888 - loss: 0.6021 - val_accuracy: 0.7857 - val_loss: 0.6109\n",
      "Epoch 171/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 65ms/step - accuracy: 0.7879 - loss: 0.6032 - val_accuracy: 0.7857 - val_loss: 0.6110\n",
      "Epoch 172/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.7879 - loss: 0.6039 - val_accuracy: 0.7870 - val_loss: 0.6084\n",
      "Epoch 173/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7885 - loss: 0.6024 - val_accuracy: 0.7866 - val_loss: 0.6086\n",
      "Epoch 174/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7882 - loss: 0.6039 - val_accuracy: 0.7848 - val_loss: 0.6131\n",
      "Epoch 175/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7890 - loss: 0.6016 - val_accuracy: 0.7867 - val_loss: 0.6084\n",
      "Epoch 176/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 63ms/step - accuracy: 0.7893 - loss: 0.6021 - val_accuracy: 0.7865 - val_loss: 0.6092\n",
      "Epoch 177/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7905 - loss: 0.5987 - val_accuracy: 0.7862 - val_loss: 0.6096\n",
      "Epoch 178/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7884 - loss: 0.6021 - val_accuracy: 0.7847 - val_loss: 0.6120\n",
      "Epoch 179/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7883 - loss: 0.6032 - val_accuracy: 0.7870 - val_loss: 0.6082\n",
      "Epoch 180/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7885 - loss: 0.6022 - val_accuracy: 0.7862 - val_loss: 0.6093\n",
      "Epoch 181/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7889 - loss: 0.6022 - val_accuracy: 0.7868 - val_loss: 0.6105\n",
      "Epoch 182/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7889 - loss: 0.6011 - val_accuracy: 0.7860 - val_loss: 0.6098\n",
      "Epoch 183/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7883 - loss: 0.6039 - val_accuracy: 0.7862 - val_loss: 0.6086\n",
      "Epoch 184/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7890 - loss: 0.6016 - val_accuracy: 0.7863 - val_loss: 0.6081\n",
      "Epoch 185/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7885 - loss: 0.6028 - val_accuracy: 0.7869 - val_loss: 0.6072\n",
      "Epoch 186/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7892 - loss: 0.6008 - val_accuracy: 0.7865 - val_loss: 0.6100\n",
      "Epoch 187/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7893 - loss: 0.5996 - val_accuracy: 0.7855 - val_loss: 0.6140\n",
      "Epoch 188/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7890 - loss: 0.6012 - val_accuracy: 0.7864 - val_loss: 0.6105\n",
      "Epoch 189/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7899 - loss: 0.5986 - val_accuracy: 0.7863 - val_loss: 0.6102\n",
      "Epoch 190/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7896 - loss: 0.5993 - val_accuracy: 0.7862 - val_loss: 0.6101\n",
      "Epoch 191/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7891 - loss: 0.6006 - val_accuracy: 0.7866 - val_loss: 0.6084\n",
      "Epoch 192/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7898 - loss: 0.5988 - val_accuracy: 0.7856 - val_loss: 0.6102\n",
      "Epoch 193/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7898 - loss: 0.6013 - val_accuracy: 0.7853 - val_loss: 0.6123\n",
      "Epoch 194/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7891 - loss: 0.5999 - val_accuracy: 0.7878 - val_loss: 0.6060\n",
      "Epoch 195/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7896 - loss: 0.5997 - val_accuracy: 0.7772 - val_loss: 0.6316\n",
      "Epoch 196/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7886 - loss: 0.6026 - val_accuracy: 0.7864 - val_loss: 0.6098\n",
      "Epoch 197/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7900 - loss: 0.5996 - val_accuracy: 0.7868 - val_loss: 0.6081\n",
      "Epoch 198/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7891 - loss: 0.6019 - val_accuracy: 0.7866 - val_loss: 0.6105\n",
      "Epoch 199/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7899 - loss: 0.6007 - val_accuracy: 0.7871 - val_loss: 0.6072\n",
      "Epoch 200/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7891 - loss: 0.6017 - val_accuracy: 0.7860 - val_loss: 0.6117\n",
      "Epoch 201/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7889 - loss: 0.6017 - val_accuracy: 0.7867 - val_loss: 0.6088\n",
      "Epoch 202/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7893 - loss: 0.6005 - val_accuracy: 0.7863 - val_loss: 0.6091\n",
      "Epoch 203/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7899 - loss: 0.5990 - val_accuracy: 0.7877 - val_loss: 0.6066\n",
      "Epoch 204/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7888 - loss: 0.6019 - val_accuracy: 0.7873 - val_loss: 0.6060\n",
      "Epoch 205/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7899 - loss: 0.6006 - val_accuracy: 0.7868 - val_loss: 0.6080\n",
      "Epoch 206/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7906 - loss: 0.5971 - val_accuracy: 0.7868 - val_loss: 0.6073\n",
      "Epoch 207/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.7903 - loss: 0.5987 - val_accuracy: 0.7866 - val_loss: 0.6075\n",
      "Epoch 208/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7898 - loss: 0.5983 - val_accuracy: 0.7869 - val_loss: 0.6084\n",
      "Epoch 209/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7898 - loss: 0.5993 - val_accuracy: 0.7862 - val_loss: 0.6085\n",
      "Epoch 210/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 411ms/step - accuracy: 0.7906 - loss: 0.5975 - val_accuracy: 0.7876 - val_loss: 0.6057\n",
      "Epoch 211/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 73ms/step - accuracy: 0.7887 - loss: 0.6025 - val_accuracy: 0.7862 - val_loss: 0.6100\n",
      "Epoch 212/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 280ms/step - accuracy: 0.7891 - loss: 0.6001 - val_accuracy: 0.7865 - val_loss: 0.6092\n",
      "Epoch 213/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7900 - loss: 0.5994 - val_accuracy: 0.7870 - val_loss: 0.6080\n",
      "Epoch 214/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 671ms/step - accuracy: 0.7908 - loss: 0.5977 - val_accuracy: 0.7845 - val_loss: 0.6144\n",
      "Epoch 215/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 126ms/step - accuracy: 0.7886 - loss: 0.6017 - val_accuracy: 0.7864 - val_loss: 0.6069\n",
      "Epoch 216/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7899 - loss: 0.5998 - val_accuracy: 0.7870 - val_loss: 0.6067\n",
      "Epoch 217/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 238ms/step - accuracy: 0.7900 - loss: 0.5980 - val_accuracy: 0.7871 - val_loss: 0.6074\n",
      "Epoch 218/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7900 - loss: 0.5981 - val_accuracy: 0.7874 - val_loss: 0.6059\n",
      "Epoch 219/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 601ms/step - accuracy: 0.7894 - loss: 0.5999 - val_accuracy: 0.7872 - val_loss: 0.6079\n",
      "Epoch 220/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7896 - loss: 0.5985 - val_accuracy: 0.7866 - val_loss: 0.6080\n",
      "Epoch 221/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 121ms/step - accuracy: 0.7908 - loss: 0.5972 - val_accuracy: 0.7868 - val_loss: 0.6064\n",
      "Epoch 222/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7895 - loss: 0.5995 - val_accuracy: 0.7861 - val_loss: 0.6096\n",
      "Epoch 223/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 154ms/step - accuracy: 0.7913 - loss: 0.5967 - val_accuracy: 0.7846 - val_loss: 0.6133\n",
      "Epoch 224/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 388ms/step - accuracy: 0.7899 - loss: 0.5987 - val_accuracy: 0.7869 - val_loss: 0.6067\n",
      "Epoch 225/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7892 - loss: 0.5991 - val_accuracy: 0.7878 - val_loss: 0.6052\n",
      "Epoch 226/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 83ms/step - accuracy: 0.7897 - loss: 0.5988 - val_accuracy: 0.7878 - val_loss: 0.6064\n",
      "Epoch 227/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7901 - loss: 0.5989 - val_accuracy: 0.7862 - val_loss: 0.6082\n",
      "Epoch 228/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 130ms/step - accuracy: 0.7906 - loss: 0.5956 - val_accuracy: 0.7859 - val_loss: 0.6084\n",
      "Epoch 229/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7900 - loss: 0.5989 - val_accuracy: 0.7827 - val_loss: 0.6183\n",
      "Epoch 230/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 257ms/step - accuracy: 0.7905 - loss: 0.5970 - val_accuracy: 0.7865 - val_loss: 0.6078\n",
      "Epoch 231/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 134ms/step - accuracy: 0.7898 - loss: 0.5987 - val_accuracy: 0.7870 - val_loss: 0.6068\n",
      "Epoch 232/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7904 - loss: 0.5975 - val_accuracy: 0.7857 - val_loss: 0.6094\n",
      "Epoch 233/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 158ms/step - accuracy: 0.7905 - loss: 0.5977 - val_accuracy: 0.7879 - val_loss: 0.6048\n",
      "Epoch 234/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7896 - loss: 0.5989 - val_accuracy: 0.7873 - val_loss: 0.6066\n",
      "Epoch 235/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 133ms/step - accuracy: 0.7900 - loss: 0.5995 - val_accuracy: 0.7866 - val_loss: 0.6063\n",
      "Epoch 236/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7899 - loss: 0.5990 - val_accuracy: 0.7870 - val_loss: 0.6074\n",
      "Epoch 237/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 120ms/step - accuracy: 0.7892 - loss: 0.5992 - val_accuracy: 0.7881 - val_loss: 0.6056\n",
      "Epoch 238/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 641ms/step - accuracy: 0.7895 - loss: 0.5986 - val_accuracy: 0.7875 - val_loss: 0.6054\n",
      "Epoch 239/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7900 - loss: 0.5979 - val_accuracy: 0.7880 - val_loss: 0.6046\n",
      "Epoch 240/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 70ms/step - accuracy: 0.7900 - loss: 0.5986 - val_accuracy: 0.7882 - val_loss: 0.6048\n",
      "Epoch 241/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7910 - loss: 0.5957 - val_accuracy: 0.7864 - val_loss: 0.6087\n",
      "Epoch 242/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 468ms/step - accuracy: 0.7911 - loss: 0.5953 - val_accuracy: 0.7867 - val_loss: 0.6067\n",
      "Epoch 243/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 125ms/step - accuracy: 0.7907 - loss: 0.5974 - val_accuracy: 0.7863 - val_loss: 0.6088\n",
      "Epoch 244/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7889 - loss: 0.6009 - val_accuracy: 0.7884 - val_loss: 0.6038\n",
      "Epoch 245/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 196ms/step - accuracy: 0.7912 - loss: 0.5955 - val_accuracy: 0.7877 - val_loss: 0.6046\n",
      "Epoch 246/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7902 - loss: 0.5973 - val_accuracy: 0.7869 - val_loss: 0.6071\n",
      "Epoch 247/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 98ms/step - accuracy: 0.7898 - loss: 0.5984 - val_accuracy: 0.7859 - val_loss: 0.6085\n",
      "Epoch 248/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7909 - loss: 0.5953 - val_accuracy: 0.7851 - val_loss: 0.6114\n",
      "Epoch 249/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 349ms/step - accuracy: 0.7901 - loss: 0.5986 - val_accuracy: 0.7876 - val_loss: 0.6043\n",
      "Epoch 250/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 90ms/step - accuracy: 0.7906 - loss: 0.5971 - val_accuracy: 0.7870 - val_loss: 0.6069\n",
      "Epoch 251/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7909 - loss: 0.5958 - val_accuracy: 0.7885 - val_loss: 0.6050\n",
      "Epoch 252/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 282ms/step - accuracy: 0.7897 - loss: 0.5982 - val_accuracy: 0.7874 - val_loss: 0.6058\n",
      "Epoch 253/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7906 - loss: 0.5964 - val_accuracy: 0.7875 - val_loss: 0.6049\n",
      "Epoch 254/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 206ms/step - accuracy: 0.7898 - loss: 0.5985 - val_accuracy: 0.7872 - val_loss: 0.6053\n",
      "Epoch 255/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7909 - loss: 0.5948 - val_accuracy: 0.7880 - val_loss: 0.6053\n",
      "Epoch 256/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7903 - loss: 0.5976 - val_accuracy: 0.7867 - val_loss: 0.6085\n",
      "Epoch 257/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7903 - loss: 0.5957 - val_accuracy: 0.7875 - val_loss: 0.6061\n",
      "Epoch 258/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7908 - loss: 0.5946 - val_accuracy: 0.7878 - val_loss: 0.6047\n",
      "Epoch 259/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7902 - loss: 0.5977 - val_accuracy: 0.7846 - val_loss: 0.6116\n",
      "Epoch 260/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7909 - loss: 0.5961 - val_accuracy: 0.7883 - val_loss: 0.6035\n",
      "Epoch 261/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7912 - loss: 0.5958 - val_accuracy: 0.7863 - val_loss: 0.6065\n",
      "Epoch 262/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7905 - loss: 0.5965 - val_accuracy: 0.7872 - val_loss: 0.6048\n",
      "Epoch 263/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7908 - loss: 0.5957 - val_accuracy: 0.7873 - val_loss: 0.6055\n",
      "Epoch 264/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7908 - loss: 0.5943 - val_accuracy: 0.7876 - val_loss: 0.6045\n",
      "Epoch 265/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.7912 - loss: 0.5945 - val_accuracy: 0.7877 - val_loss: 0.6045\n",
      "Epoch 266/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.7898 - loss: 0.5988 - val_accuracy: 0.7855 - val_loss: 0.6101\n",
      "Epoch 267/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7904 - loss: 0.5968 - val_accuracy: 0.7885 - val_loss: 0.6041\n",
      "Epoch 268/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7899 - loss: 0.5966 - val_accuracy: 0.7887 - val_loss: 0.6032\n",
      "Epoch 269/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7905 - loss: 0.5959 - val_accuracy: 0.7879 - val_loss: 0.6041\n",
      "Epoch 270/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7914 - loss: 0.5933 - val_accuracy: 0.7888 - val_loss: 0.6033\n",
      "Epoch 271/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7908 - loss: 0.5953 - val_accuracy: 0.7876 - val_loss: 0.6069\n",
      "Epoch 272/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7915 - loss: 0.5947 - val_accuracy: 0.7873 - val_loss: 0.6042\n",
      "Epoch 273/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7906 - loss: 0.5955 - val_accuracy: 0.7828 - val_loss: 0.6175\n",
      "Epoch 274/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7899 - loss: 0.5976 - val_accuracy: 0.7875 - val_loss: 0.6056\n",
      "Epoch 275/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7913 - loss: 0.5948 - val_accuracy: 0.7885 - val_loss: 0.6034\n",
      "Epoch 276/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 90ms/step - accuracy: 0.7900 - loss: 0.5976 - val_accuracy: 0.7885 - val_loss: 0.6039\n",
      "Epoch 277/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 187ms/step - accuracy: 0.7908 - loss: 0.5952 - val_accuracy: 0.7876 - val_loss: 0.6049\n",
      "Epoch 278/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7921 - loss: 0.5943 - val_accuracy: 0.7881 - val_loss: 0.6043\n",
      "Epoch 279/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 239ms/step - accuracy: 0.7923 - loss: 0.5918 - val_accuracy: 0.7875 - val_loss: 0.6041\n",
      "Epoch 280/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7914 - loss: 0.5949 - val_accuracy: 0.7886 - val_loss: 0.6033\n",
      "Epoch 281/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 85ms/step - accuracy: 0.7912 - loss: 0.5942 - val_accuracy: 0.7861 - val_loss: 0.6098\n",
      "Epoch 282/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 82ms/step - accuracy: 0.7912 - loss: 0.5945 - val_accuracy: 0.7845 - val_loss: 0.6135\n",
      "Epoch 283/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 333ms/step - accuracy: 0.7900 - loss: 0.5975 - val_accuracy: 0.7845 - val_loss: 0.6128\n",
      "Epoch 284/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 159ms/step - accuracy: 0.7911 - loss: 0.5946 - val_accuracy: 0.7878 - val_loss: 0.6030\n",
      "Epoch 285/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7905 - loss: 0.5955 - val_accuracy: 0.7844 - val_loss: 0.6119\n",
      "Epoch 286/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.7896 - loss: 0.5964 - val_accuracy: 0.7874 - val_loss: 0.6054\n",
      "Epoch 287/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 391ms/step - accuracy: 0.7916 - loss: 0.5942 - val_accuracy: 0.7862 - val_loss: 0.6095\n",
      "Epoch 288/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7906 - loss: 0.5949 - val_accuracy: 0.7880 - val_loss: 0.6036\n",
      "Epoch 289/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7906 - loss: 0.5967 - val_accuracy: 0.7880 - val_loss: 0.6045\n",
      "Epoch 290/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7910 - loss: 0.5951 - val_accuracy: 0.7878 - val_loss: 0.6040\n",
      "Epoch 291/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7903 - loss: 0.5962 - val_accuracy: 0.7881 - val_loss: 0.6025\n",
      "Epoch 292/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7908 - loss: 0.5946 - val_accuracy: 0.7864 - val_loss: 0.6078\n",
      "Epoch 293/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 61ms/step - accuracy: 0.7916 - loss: 0.5947 - val_accuracy: 0.7874 - val_loss: 0.6045\n",
      "Epoch 294/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7910 - loss: 0.5928 - val_accuracy: 0.7881 - val_loss: 0.6030\n",
      "Epoch 295/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7915 - loss: 0.5931 - val_accuracy: 0.7883 - val_loss: 0.6028\n",
      "Epoch 296/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7906 - loss: 0.5957 - val_accuracy: 0.7849 - val_loss: 0.6101\n",
      "Epoch 297/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 61ms/step - accuracy: 0.7914 - loss: 0.5928 - val_accuracy: 0.7879 - val_loss: 0.6037\n",
      "Epoch 298/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7907 - loss: 0.5951 - val_accuracy: 0.7888 - val_loss: 0.6027\n",
      "Epoch 299/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7907 - loss: 0.5953 - val_accuracy: 0.7871 - val_loss: 0.6059\n",
      "Epoch 300/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7909 - loss: 0.5947 - val_accuracy: 0.7876 - val_loss: 0.6033\n",
      "Epoch 301/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 67ms/step - accuracy: 0.7907 - loss: 0.5948 - val_accuracy: 0.7873 - val_loss: 0.6056\n",
      "Epoch 302/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7911 - loss: 0.5952 - val_accuracy: 0.7878 - val_loss: 0.6041\n",
      "Epoch 303/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7912 - loss: 0.5945 - val_accuracy: 0.7883 - val_loss: 0.6024\n",
      "Epoch 304/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 70ms/step - accuracy: 0.7914 - loss: 0.5941 - val_accuracy: 0.7885 - val_loss: 0.6033\n",
      "Epoch 305/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 69ms/step - accuracy: 0.7916 - loss: 0.5938 - val_accuracy: 0.7874 - val_loss: 0.6039\n",
      "Epoch 306/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7917 - loss: 0.5947 - val_accuracy: 0.7878 - val_loss: 0.6033\n",
      "Epoch 307/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 69ms/step - accuracy: 0.7909 - loss: 0.5950 - val_accuracy: 0.7888 - val_loss: 0.6021\n",
      "Epoch 308/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 69ms/step - accuracy: 0.7915 - loss: 0.5949 - val_accuracy: 0.7876 - val_loss: 0.6038\n",
      "Epoch 309/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7915 - loss: 0.5958 - val_accuracy: 0.7878 - val_loss: 0.6032\n",
      "Epoch 310/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7912 - loss: 0.5951 - val_accuracy: 0.7878 - val_loss: 0.6031\n",
      "Epoch 311/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7899 - loss: 0.5972 - val_accuracy: 0.7878 - val_loss: 0.6031\n",
      "Epoch 312/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7905 - loss: 0.5955 - val_accuracy: 0.7871 - val_loss: 0.6056\n",
      "Epoch 313/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7913 - loss: 0.5947 - val_accuracy: 0.7884 - val_loss: 0.6030\n",
      "Epoch 314/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7904 - loss: 0.5959 - val_accuracy: 0.7881 - val_loss: 0.6028\n",
      "Epoch 315/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7908 - loss: 0.5943 - val_accuracy: 0.7880 - val_loss: 0.6051\n",
      "Epoch 316/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7910 - loss: 0.5954 - val_accuracy: 0.7886 - val_loss: 0.6019\n",
      "Epoch 317/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7915 - loss: 0.5930 - val_accuracy: 0.7886 - val_loss: 0.6025\n",
      "Epoch 318/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7920 - loss: 0.5935 - val_accuracy: 0.7887 - val_loss: 0.6024\n",
      "Epoch 319/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7908 - loss: 0.5939 - val_accuracy: 0.7884 - val_loss: 0.6024\n",
      "Epoch 320/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7917 - loss: 0.5935 - val_accuracy: 0.7881 - val_loss: 0.6028\n",
      "Epoch 321/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7914 - loss: 0.5949 - val_accuracy: 0.7864 - val_loss: 0.6089\n",
      "Epoch 322/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7922 - loss: 0.5934 - val_accuracy: 0.7879 - val_loss: 0.6041\n",
      "Epoch 323/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7922 - loss: 0.5928 - val_accuracy: 0.7854 - val_loss: 0.6091\n",
      "Epoch 324/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7914 - loss: 0.5933 - val_accuracy: 0.7883 - val_loss: 0.6026\n",
      "Epoch 325/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7908 - loss: 0.5960 - val_accuracy: 0.7872 - val_loss: 0.6055\n",
      "Epoch 326/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7915 - loss: 0.5948 - val_accuracy: 0.7883 - val_loss: 0.6050\n",
      "Epoch 327/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7906 - loss: 0.5954 - val_accuracy: 0.7883 - val_loss: 0.6028\n",
      "Epoch 328/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7905 - loss: 0.5959 - val_accuracy: 0.7891 - val_loss: 0.6010\n",
      "Epoch 329/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7928 - loss: 0.5920 - val_accuracy: 0.7882 - val_loss: 0.6020\n",
      "Epoch 330/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7912 - loss: 0.5953 - val_accuracy: 0.7880 - val_loss: 0.6064\n",
      "Epoch 331/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7919 - loss: 0.5938 - val_accuracy: 0.7869 - val_loss: 0.6069\n",
      "Epoch 332/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7915 - loss: 0.5930 - val_accuracy: 0.7865 - val_loss: 0.6086\n",
      "Epoch 333/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7904 - loss: 0.5948 - val_accuracy: 0.7855 - val_loss: 0.6099\n",
      "Epoch 334/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7923 - loss: 0.5925 - val_accuracy: 0.7886 - val_loss: 0.6020\n",
      "Epoch 335/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7905 - loss: 0.5960 - val_accuracy: 0.7886 - val_loss: 0.6005\n",
      "Epoch 336/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 695ms/step - accuracy: 0.7915 - loss: 0.5928 - val_accuracy: 0.7888 - val_loss: 0.6033\n",
      "Epoch 337/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7915 - loss: 0.5931 - val_accuracy: 0.7891 - val_loss: 0.6011\n",
      "Epoch 338/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 135ms/step - accuracy: 0.7924 - loss: 0.5926 - val_accuracy: 0.7882 - val_loss: 0.6042\n",
      "Epoch 339/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7917 - loss: 0.5930 - val_accuracy: 0.7889 - val_loss: 0.6024\n",
      "Epoch 340/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 582ms/step - accuracy: 0.7916 - loss: 0.5930 - val_accuracy: 0.7888 - val_loss: 0.6016\n",
      "Epoch 341/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.7924 - loss: 0.5926 - val_accuracy: 0.7887 - val_loss: 0.6015\n",
      "Epoch 342/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 857ms/step - accuracy: 0.7910 - loss: 0.5936 - val_accuracy: 0.7883 - val_loss: 0.6016\n",
      "Epoch 343/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 266ms/step - accuracy: 0.7912 - loss: 0.5941 - val_accuracy: 0.7887 - val_loss: 0.6018\n",
      "Epoch 344/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7920 - loss: 0.5916 - val_accuracy: 0.7877 - val_loss: 0.6055\n",
      "Epoch 345/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 195ms/step - accuracy: 0.7913 - loss: 0.5928 - val_accuracy: 0.7881 - val_loss: 0.6053\n",
      "Epoch 346/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 70ms/step - accuracy: 0.7915 - loss: 0.5934 - val_accuracy: 0.7885 - val_loss: 0.6016\n",
      "Epoch 347/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 621ms/step - accuracy: 0.7914 - loss: 0.5930 - val_accuracy: 0.7895 - val_loss: 0.5998\n",
      "Epoch 348/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 199ms/step - accuracy: 0.7919 - loss: 0.5925 - val_accuracy: 0.7879 - val_loss: 0.6038\n",
      "Epoch 349/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7919 - loss: 0.5927 - val_accuracy: 0.7887 - val_loss: 0.6029\n",
      "Epoch 350/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7911 - loss: 0.5928 - val_accuracy: 0.7883 - val_loss: 0.6017\n",
      "Epoch 351/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7917 - loss: 0.5917 - val_accuracy: 0.7888 - val_loss: 0.6011\n",
      "Epoch 352/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.7912 - loss: 0.5941 - val_accuracy: 0.7870 - val_loss: 0.6058\n",
      "Epoch 353/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7916 - loss: 0.5935 - val_accuracy: 0.7888 - val_loss: 0.6021\n",
      "Epoch 354/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7921 - loss: 0.5919 - val_accuracy: 0.7878 - val_loss: 0.6018\n",
      "Epoch 355/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7914 - loss: 0.5933 - val_accuracy: 0.7886 - val_loss: 0.6023\n",
      "Epoch 356/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7921 - loss: 0.5929 - val_accuracy: 0.7872 - val_loss: 0.6042\n",
      "Epoch 357/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7915 - loss: 0.5935 - val_accuracy: 0.7882 - val_loss: 0.6031\n",
      "Epoch 358/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7931 - loss: 0.5898 - val_accuracy: 0.7875 - val_loss: 0.6058\n",
      "Epoch 359/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7924 - loss: 0.5915 - val_accuracy: 0.7885 - val_loss: 0.6017\n",
      "Epoch 360/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.7925 - loss: 0.5916 - val_accuracy: 0.7889 - val_loss: 0.6018\n",
      "Epoch 361/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7921 - loss: 0.5929 - val_accuracy: 0.7870 - val_loss: 0.6060\n",
      "Epoch 362/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7918 - loss: 0.5939 - val_accuracy: 0.7887 - val_loss: 0.6017\n",
      "Epoch 363/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - accuracy: 0.7909 - loss: 0.5958 - val_accuracy: 0.7876 - val_loss: 0.6032\n",
      "Epoch 364/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.7929 - loss: 0.5909 - val_accuracy: 0.7875 - val_loss: 0.6038\n",
      "Epoch 365/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 63ms/step - accuracy: 0.7906 - loss: 0.5955 - val_accuracy: 0.7878 - val_loss: 0.6051\n",
      "Epoch 366/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7920 - loss: 0.5921 - val_accuracy: 0.7883 - val_loss: 0.6009\n",
      "Epoch 367/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7925 - loss: 0.5926 - val_accuracy: 0.7891 - val_loss: 0.6013\n",
      "Epoch 368/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7924 - loss: 0.5927 - val_accuracy: 0.7884 - val_loss: 0.6003\n",
      "Epoch 369/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 69ms/step - accuracy: 0.7912 - loss: 0.5934 - val_accuracy: 0.7846 - val_loss: 0.6134\n",
      "Epoch 370/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 68ms/step - accuracy: 0.7915 - loss: 0.5918 - val_accuracy: 0.7884 - val_loss: 0.6009\n",
      "Epoch 371/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7913 - loss: 0.5932 - val_accuracy: 0.7885 - val_loss: 0.6022\n",
      "Epoch 372/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7915 - loss: 0.5928 - val_accuracy: 0.7877 - val_loss: 0.6051\n",
      "Epoch 373/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7913 - loss: 0.5938 - val_accuracy: 0.7880 - val_loss: 0.6028\n",
      "Epoch 374/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7922 - loss: 0.5919 - val_accuracy: 0.7893 - val_loss: 0.6002\n",
      "Epoch 375/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7930 - loss: 0.5901 - val_accuracy: 0.7880 - val_loss: 0.6024\n",
      "Epoch 376/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7921 - loss: 0.5929 - val_accuracy: 0.7859 - val_loss: 0.6071\n",
      "Epoch 377/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7919 - loss: 0.5924 - val_accuracy: 0.7872 - val_loss: 0.6039\n",
      "Epoch 378/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 67ms/step - accuracy: 0.7912 - loss: 0.5931 - val_accuracy: 0.7883 - val_loss: 0.6011\n",
      "Epoch 379/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7926 - loss: 0.5906 - val_accuracy: 0.7883 - val_loss: 0.6018\n",
      "Epoch 380/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7919 - loss: 0.5921 - val_accuracy: 0.7888 - val_loss: 0.6008\n",
      "Epoch 381/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 67ms/step - accuracy: 0.7919 - loss: 0.5913 - val_accuracy: 0.7888 - val_loss: 0.6002\n",
      "Epoch 382/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7914 - loss: 0.5936 - val_accuracy: 0.7890 - val_loss: 0.6014\n",
      "Epoch 383/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7921 - loss: 0.5917 - val_accuracy: 0.7875 - val_loss: 0.6028\n",
      "Epoch 384/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.7917 - loss: 0.5921 - val_accuracy: 0.7886 - val_loss: 0.6014\n",
      "Epoch 385/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.7920 - loss: 0.5921 - val_accuracy: 0.7891 - val_loss: 0.5997\n",
      "Epoch 386/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.7916 - loss: 0.5919 - val_accuracy: 0.7891 - val_loss: 0.6005\n",
      "Epoch 387/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.7923 - loss: 0.5911 - val_accuracy: 0.7888 - val_loss: 0.6022\n",
      "Epoch 388/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.7917 - loss: 0.5921 - val_accuracy: 0.7894 - val_loss: 0.5995\n",
      "Epoch 389/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.7921 - loss: 0.5908 - val_accuracy: 0.7886 - val_loss: 0.6019\n",
      "Epoch 390/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 63ms/step - accuracy: 0.7922 - loss: 0.5920 - val_accuracy: 0.7886 - val_loss: 0.6034\n",
      "Epoch 391/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7913 - loss: 0.5939 - val_accuracy: 0.7868 - val_loss: 0.6067\n",
      "Epoch 392/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.7921 - loss: 0.5913 - val_accuracy: 0.7878 - val_loss: 0.6028\n",
      "Epoch 393/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 65ms/step - accuracy: 0.7923 - loss: 0.5923 - val_accuracy: 0.7893 - val_loss: 0.6000\n",
      "Epoch 394/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 66ms/step - accuracy: 0.7925 - loss: 0.5902 - val_accuracy: 0.7867 - val_loss: 0.6076\n",
      "Epoch 395/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7922 - loss: 0.5923 - val_accuracy: 0.7889 - val_loss: 0.6005\n",
      "Epoch 396/500\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.7923 - loss: 0.5925 - val_accuracy: 0.7883 - val_loss: 0.6006\n",
      "Epoch 397/500\n",
      "\u001b[1m127/485\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 63ms/step - accuracy: 0.7946 - loss: 0.5842"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1498\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1509\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1510\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1515\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=1024\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('transformer2Layer32.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anrunw/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 74 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('transformer2Layer32.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_softmax16noLN.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('model_softmax.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anrunw/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'linformer_transformer_block', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/anrunw/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'linformer_transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_1   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,765</span> (22.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,765\u001b[0m (22.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,765</span> (22.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,765\u001b[0m (22.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregation Layer\n",
    "# ---------------------------\n",
    "class AggregationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Aggregates a set of features over the sequence dimension.\n",
    "    Supported aggregations: \"mean\" or \"max\".\n",
    "    \"\"\"\n",
    "    def __init__(self, aggreg=\"mean\", **kwargs):\n",
    "        super(AggregationLayer, self).__init__(**kwargs)\n",
    "        self.aggreg = aggreg\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.aggreg == \"mean\":\n",
    "            return tf.reduce_mean(inputs, axis=1)\n",
    "        elif self.aggreg == \"max\":\n",
    "            return tf.reduce_max(inputs, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Given aggregation string is not implemented. Use 'mean' or 'max'.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Linformer Multi-Head Attention\n",
    "# ---------------------------\n",
    "class LinformerMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, proj_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          d_model: Dimensionality of the model.\n",
    "          num_heads: Number of attention heads.\n",
    "          proj_dim: The projection dimension to which keys and values will be reduced.\n",
    "        \"\"\"\n",
    "        super(LinformerMultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.proj_dim = proj_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len, d_model)\n",
    "        self.seq_len = input_shape[1]\n",
    "        # Standard dense weight matrices for Q, K, and V.\n",
    "        self.wq = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wq\")\n",
    "        self.wk = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wk\")\n",
    "        self.wv = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wv\")\n",
    "        self.dense = layers.Dense(self.d_model)\n",
    "        # Learnable projection matrices for keys and values.\n",
    "        # These project along the sequence dimension from seq_len -> proj_dim.\n",
    "        self.E = self.add_weight(shape=(self.num_heads, self.seq_len, self.proj_dim),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 name=\"proj_E\")\n",
    "        self.F = self.add_weight(shape=(self.num_heads, self.seq_len, self.proj_dim),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 name=\"proj_F\")\n",
    "        super(LinformerMultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        # Transpose to shape: (batch_size, num_heads, seq_len, depth)\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Compute linear projections.\n",
    "        q = tf.matmul(x, self.wq)  # (batch_size, seq_len, d_model)\n",
    "        k = tf.matmul(x, self.wk)\n",
    "        v = tf.matmul(x, self.wv)\n",
    "\n",
    "        # Split into multiple heads.\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Project keys and values along the sequence dimension.\n",
    "        # Using Einstein summation: for each head h,\n",
    "        #   k_proj[b, h] = E[h]^T (applied on the sequence dim of k[b, h])\n",
    "        # k_proj shape: (batch_size, num_heads, proj_dim, depth)\n",
    "        k_proj = tf.einsum('bhnd, hnr -> bhrd', k, self.E)\n",
    "        v_proj = tf.einsum('bhnd, hnr -> bhrd', v, self.F)\n",
    "\n",
    "        # Scaled dot-product attention.\n",
    "        # Compute scores between queries and projected keys.\n",
    "        # scores shape: (batch_size, num_heads, seq_len, proj_dim)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        scores = tf.matmul(q, k_proj, transpose_b=True) / tf.math.sqrt(dk)\n",
    "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
    "\n",
    "        # Compute the attention output.\n",
    "        # Output shape: (batch_size, num_heads, seq_len, depth)\n",
    "        attn_output = tf.matmul(attn_weights, v_proj)\n",
    "\n",
    "        # Concatenate heads.\n",
    "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])\n",
    "        concat_output = tf.reshape(attn_output, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_output)\n",
    "        return output\n",
    "\n",
    "# ---------------------------\n",
    "# Linformer Transformer Block\n",
    "# ---------------------------\n",
    "class LinformerTransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, output_dim, num_heads, proj_dim, **kwargs):\n",
    "        super(LinformerTransformerBlock, self).__init__(**kwargs)\n",
    "        self.attention = LinformerMultiHeadAttention(d_model, num_heads, proj_dim)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.outD = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "# ---------------------------\n",
    "# Linformer Transformer Classifier Model\n",
    "# ---------------------------\n",
    "def build_linformer_transformer_classifier(num_particles, feature_dim,\n",
    "                                             d_model=16, d_ff=16, output_dim=16,\n",
    "                                             num_heads=1, proj_dim=8):\n",
    "    \"\"\"\n",
    "    Builds a classifier model with:\n",
    "      - A linear embedding layer.\n",
    "      - Multiple Linformer transformer blocks.\n",
    "      - Aggregation over the sequence dimension.\n",
    "      - A final linear output layer for 5 classes.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_particles, feature_dim))\n",
    "    \n",
    "    # Linear embedding.\n",
    "    x = layers.Dense(d_model, activation='relu')(inputs)\n",
    "    \n",
    "    # Apply multiple Linformer transformer blocks.\n",
    "    x = LinformerTransformerBlock(d_model, d_ff, output_dim, num_heads, proj_dim)(x)\n",
    "    x = LinformerTransformerBlock(d_model, d_ff, output_dim, num_heads, proj_dim)(x)\n",
    "    \n",
    "    # Aggregation (using \"max\" aggregation).\n",
    "    pooled_output = AggregationLayer(aggreg='max')(x)\n",
    "    \n",
    "    # Final dense layers.\n",
    "    x = layers.Dense(d_model, activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# ---------------------------\n",
    "# Example Training Setup\n",
    "# ---------------------------\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data.\n",
    "x_train = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/x_train_robust_16const_ptetaphi.npy\")\n",
    "y_train = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/y_train_robust_16const_ptetaphi.npy\")\n",
    "\n",
    "# 80/20 split for training and validation.\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Determine input dimensions from data.\n",
    "num_particles = x_train.shape[1]\n",
    "feature_dim = x_train.shape[-1]\n",
    "\n",
    "# Build and compile the Linformer transformer classifier.\n",
    "model = build_linformer_transformer_classifier(num_particles, feature_dim,\n",
    "                                               d_model=16, d_ff=16, output_dim=16,\n",
    "                                               num_heads=8, proj_dim=4)\n",
    "model.load_weights('linformer.weights.h5')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_45\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_45\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_169 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_26  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_27  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_178 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_179 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_45 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_169 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_26  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_27  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_178 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_179 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,765</span> (22.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,765\u001b[0m (22.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,765</span> (22.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,765\u001b[0m (22.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - accuracy: 0.7224 - loss: 0.7656 - val_accuracy: 0.7174 - val_loss: 0.7785\n",
      "Epoch 2/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.7232 - loss: 0.7643 - val_accuracy: 0.7110 - val_loss: 0.7907\n",
      "Epoch 3/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 34ms/step - accuracy: 0.7207 - loss: 0.7683 - val_accuracy: 0.7165 - val_loss: 0.7784\n",
      "Epoch 4/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.7219 - loss: 0.7667 - val_accuracy: 0.7169 - val_loss: 0.7785\n",
      "Epoch 5/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7241 - loss: 0.7612 - val_accuracy: 0.7168 - val_loss: 0.7784\n",
      "Epoch 6/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7223 - loss: 0.7643 - val_accuracy: 0.7167 - val_loss: 0.7793\n",
      "Epoch 7/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.7215 - loss: 0.7659 - val_accuracy: 0.7101 - val_loss: 0.7943\n",
      "Epoch 8/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7221 - loss: 0.7666 - val_accuracy: 0.7192 - val_loss: 0.7735\n",
      "Epoch 9/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7231 - loss: 0.7634 - val_accuracy: 0.7094 - val_loss: 0.7964\n",
      "Epoch 10/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.7242 - loss: 0.7615 - val_accuracy: 0.7180 - val_loss: 0.7753\n",
      "Epoch 11/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7238 - loss: 0.7610 - val_accuracy: 0.7079 - val_loss: 0.7967\n",
      "Epoch 12/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7228 - loss: 0.7629 - val_accuracy: 0.7184 - val_loss: 0.7779\n",
      "Epoch 13/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7231 - loss: 0.7636 - val_accuracy: 0.7136 - val_loss: 0.7870\n",
      "Epoch 14/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7230 - loss: 0.7641 - val_accuracy: 0.7187 - val_loss: 0.7745\n",
      "Epoch 15/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7241 - loss: 0.7622 - val_accuracy: 0.7138 - val_loss: 0.7839\n",
      "Epoch 16/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7229 - loss: 0.7646 - val_accuracy: 0.7205 - val_loss: 0.7694\n",
      "Epoch 17/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7242 - loss: 0.7601 - val_accuracy: 0.7146 - val_loss: 0.7820\n",
      "Epoch 18/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7239 - loss: 0.7614 - val_accuracy: 0.7192 - val_loss: 0.7724\n",
      "Epoch 19/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7241 - loss: 0.7611 - val_accuracy: 0.7157 - val_loss: 0.7825\n",
      "Epoch 20/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7239 - loss: 0.7616 - val_accuracy: 0.7204 - val_loss: 0.7717\n",
      "Epoch 21/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.7248 - loss: 0.7598 - val_accuracy: 0.7155 - val_loss: 0.7810\n",
      "Epoch 22/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.7251 - loss: 0.7604 - val_accuracy: 0.7211 - val_loss: 0.7699\n",
      "Epoch 23/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.7250 - loss: 0.7590 - val_accuracy: 0.7202 - val_loss: 0.7719\n",
      "Epoch 24/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7240 - loss: 0.7616 - val_accuracy: 0.7180 - val_loss: 0.7755\n",
      "Epoch 25/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.7240 - loss: 0.7603 - val_accuracy: 0.7146 - val_loss: 0.7846\n",
      "Epoch 26/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7242 - loss: 0.7599 - val_accuracy: 0.7165 - val_loss: 0.7784\n",
      "Epoch 27/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7247 - loss: 0.7599 - val_accuracy: 0.7211 - val_loss: 0.7737\n",
      "Epoch 28/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7252 - loss: 0.7575 - val_accuracy: 0.7197 - val_loss: 0.7716\n",
      "Epoch 29/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.7245 - loss: 0.7592 - val_accuracy: 0.7197 - val_loss: 0.7710\n",
      "Epoch 30/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7243 - loss: 0.7600 - val_accuracy: 0.7160 - val_loss: 0.7832\n",
      "Epoch 31/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.7241 - loss: 0.7605 - val_accuracy: 0.7166 - val_loss: 0.7783\n",
      "Epoch 32/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7262 - loss: 0.7573 - val_accuracy: 0.7215 - val_loss: 0.7698\n",
      "Epoch 33/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.7256 - loss: 0.7575 - val_accuracy: 0.7150 - val_loss: 0.7832\n",
      "Epoch 34/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.7251 - loss: 0.7577 - val_accuracy: 0.7228 - val_loss: 0.7661\n",
      "Epoch 35/100\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7260 - loss: 0.7567 - val_accuracy: 0.7201 - val_loss: 0.7701\n",
      "Epoch 36/100\n",
      "\u001b[1m432/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.7251 - loss: 0.7580"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1498\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1508\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1509\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1510\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1515\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=1024\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('linformer.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Suppose you have already trained your model:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# history = model.fit(...)\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Plotting the training & validation loss values\u001b[39;00m\n\u001b[32m      7\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m plt.plot(\u001b[43mhistory\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m history.history:\n\u001b[32m     10\u001b[39m     plt.plot(history.history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mValidation Loss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose you have already trained your model:\n",
    "# history = model.fit(...)\n",
    "\n",
    "# Plotting the training & validation loss values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# If you also have accuracy or other metrics, you can plot them similarly:\n",
    "if 'accuracy' in history.history:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('linformer.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU(s):\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_110\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_110\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_495 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_78            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_39            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_501 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_120 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_495 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_78            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,128\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_39            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_501 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_502 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549</span> (9.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,549\u001b[0m (9.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549</span> (9.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,549\u001b[0m (9.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Check for available GPUs.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Using GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs found. Ensure tensorflow-macos and tensorflow-metal are installed for GPU acceleration on M2 Macs.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Gumbel Softmax Layer with Hard Sampling\n",
    "# ---------------------------\n",
    "class GumbelSoftmax(layers.Layer):\n",
    "\n",
    "    def __init__(self, temperature=1.0, hard=False, **kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = tf.random.uniform(shape, minval=0, maxval=1)\n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def call(self, logits):\n",
    "        noise = self.sample_gumbel(tf.shape(logits))\n",
    "        y = logits + noise\n",
    "        y_soft = tf.nn.softmax(y / self.temperature, axis=-1)\n",
    "        if self.hard:\n",
    "            y_hard = tf.one_hot(tf.argmax(y_soft, axis=-1), depth=tf.shape(y_soft)[-1])\n",
    "            y = tf.stop_gradient(y_hard - y_soft) + y_soft\n",
    "            return y\n",
    "        else:\n",
    "            return y_soft\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregation Layer\n",
    "# ---------------------------\n",
    "class AggregationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Aggregates a set of features over the sequence dimension.\n",
    "    Supported aggregations: \"mean\" or \"max\".\n",
    "    \"\"\"\n",
    "    def __init__(self, aggreg=\"mean\", **kwargs):\n",
    "        super(AggregationLayer, self).__init__(**kwargs)\n",
    "        self.aggreg = aggreg\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.aggreg == \"mean\":\n",
    "            return tf.reduce_mean(inputs, axis=1)\n",
    "        elif self.aggreg == \"max\":\n",
    "            return tf.reduce_max(inputs, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Given aggregation string is not implemented. Use 'mean' or 'max'.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Custom Multi-Head Attention with Gumbel Softmax\n",
    "# ---------------------------\n",
    "class CustomMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, attn_temperature=1.0, **kwargs):\n",
    "        super(CustomMultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.attn_temperature = attn_temperature\n",
    "        self.attn_matrix = []\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Define weight matrices for Q, K, V.\n",
    "        self.wq = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wq\")\n",
    "        self.wk = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wk\")\n",
    "        self.wv = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wv\")\n",
    "        self.dense = layers.Dense(self.d_model)\n",
    "        super(CustomMultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Reshape x to (batch_size, seq_len, num_heads, depth) then transpose.\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Compute Q, K, V.\n",
    "        q = tf.matmul(x, self.wq)  # (batch_size, seq_len, d_model)\n",
    "        k = tf.matmul(x, self.wk)\n",
    "        v = tf.matmul(x, self.wv)\n",
    "\n",
    "        # Split into multiple heads.\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Compute scaled dot-product attention logits.\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply Gumbel Softmax (with hard sampling) instead of standard softmax.\n",
    "        gumbel_layer = GumbelSoftmax(temperature=self.attn_temperature, hard=True)\n",
    "        attention_weights = gumbel_layer(scaled_attention_logits)\n",
    "        # Compute attention output.\n",
    "        scaled_attention = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Concatenate heads.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Final linear layer.\n",
    "        output = self.dense(concat_attention) \n",
    "        self.attn_matrix.append(attention_weights)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Block\n",
    "# ---------------------------\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, output_dim, num_heads, attn_temperature=1.0, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.mha = CustomMultiHeadAttention(d_model, num_heads, attn_temperature=attn_temperature)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.outD = layers.Dense(16)\n",
    "        self.outD2 = layers.Dense(16)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Multi-head attention sub-layer.\n",
    "        attn_output = self.mha(x)[0]\n",
    "        out1 = self.outD(x + attn_output)\n",
    "        # Feed-forward sub-layer.\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = out1 + ffn_output\n",
    "        out2 = self.outD2(out2)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definition\n",
    "# ---------------------------\n",
    "def build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                          d_model=16, d_ff=16, output_dim=16,\n",
    "                                          num_heads=1, attn_temperature=0.5):\n",
    "    \"\"\"\n",
    "    Builds a classifier model with:\n",
    "      - A linear embedding layer.\n",
    "      - Multiple transformer encoder blocks using Gumbel Softmax in attention.\n",
    "      - Aggregation over the sequence dimension using AggregationLayer.\n",
    "      - A final linear output layer for 5 classes.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_particles, feature_dim))\n",
    "    \n",
    "    # Linear embedding.\n",
    "    x = layers.Dense(16, activation='relu')(inputs)\n",
    "\n",
    "    # Apply multiple transformer blocks.\n",
    "    x = TransformerBlock(16, 16, output_dim=8, num_heads=8, attn_temperature=attn_temperature)(x)\n",
    "\n",
    "    \n",
    "    # Custom aggregation (using \"max\" aggregation here).\n",
    "    pooled_output = AggregationLayer(aggreg='max')(x)\n",
    "    \n",
    "    # Final dense layers.\n",
    "    x = layers.Dense(16, activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "import numpy as np\n",
    "\n",
    "# Load your data.\n",
    "# Determine input dimensions.\n",
    "num_particles = x_train.shape[1]\n",
    "feature_dim = x_train.shape[-1]\n",
    "\n",
    "# Build and compile the model.\n",
    "model = build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                            d_model=8, d_ff=8,\n",
    "                                            output_dim=8, num_heads=4,\n",
    "                                            attn_temperature= 0.1)\n",
    "model.load_weights('model_gumbel_softmax16noLN.weights.h5')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Print model summary.\n",
    "model.summary()\n",
    "\n",
    "# Train the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1068564517.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel.\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m 61/122\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 163ms/step - accuracy: 0.6937 - loss: 0.8362 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/zj/988sh2h5501gdntnynlhn0480000gn/T/ipykernel_13953/985765736.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = model.fit(x_train, y_train,\n\u001b[32m      2\u001b[39m                     validation_data=(x_val, y_val),\n\u001b[32m      3\u001b[39m                     epochs=\u001b[32m1000\u001b[39m,\n\u001b[32m      4\u001b[39m                     batch_size=\u001b[32m4096\u001b[39m\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    367\u001b[39m             callbacks.on_epoch_begin(epoch)\n\u001b[32m    368\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m    369\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;28;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m                     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m                     logs = self.train_function(iterator)\n\u001b[32m    372\u001b[39m                     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m self.stop_training:\n\u001b[32m    374\u001b[39m                         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m function(iterator):\n\u001b[32m    216\u001b[39m             if isinstance(\n\u001b[32m    217\u001b[39m                 iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m             ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m                 opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m    220\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m StopIteration\n\u001b[32m    222\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    806\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, *args, **kwds):\n\u001b[32m    807\u001b[39m     \u001b[38;5;66;03m# Implements PolymorphicFunction.__call__.\u001b[39;00m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m self._run_functions_eagerly:\n\u001b[32m    809\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(self._name, tf_function_call=\u001b[33m\"eager\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._python_function(*args, **kwds)\n\u001b[32m    811\u001b[39m \n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m     \u001b[38;5;66;03m# place.\u001b[39;00m\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    128\u001b[39m         @tf.autograph.experimental.do_not_convert\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m multi_step_on_iterator(iterator):\n\u001b[32m    130\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    131\u001b[39m                 return tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m                     one_step_on_data(iterator.get_next())\n\u001b[32m    133\u001b[39m                 )\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m             \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    806\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, *args, **kwds):\n\u001b[32m    807\u001b[39m     \u001b[38;5;66;03m# Implements PolymorphicFunction.__call__.\u001b[39;00m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m self._run_functions_eagerly:\n\u001b[32m    809\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(self._name, tf_function_call=\u001b[33m\"eager\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._python_function(*args, **kwds)\n\u001b[32m    811\u001b[39m \n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m     \u001b[38;5;66;03m# place.\u001b[39;00m\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    110\u001b[39m         @tf.autograph.experimental.do_not_convert\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m one_step_on_data(data):\n\u001b[32m    112\u001b[39m             \u001b[33m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m             outputs = self.distribute_strategy.run(step_function, args=(data,))\n\u001b[32m    114\u001b[39m             outputs = reduce_per_replica(\n\u001b[32m    115\u001b[39m                 outputs,\n\u001b[32m    116\u001b[39m                 self.distribute_strategy,\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1669\u001b[39m       \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m       \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3259\u001b[39m     _require_cross_replica_or_default_context_extended(self)\n\u001b[32m   3260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3261\u001b[39m       kwargs = {}\n\u001b[32m   3262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self._call_for_each_replica(fn, args, kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _call_for_each_replica(self, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(self._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     76\u001b[39m             trainable_weights = self.trainable_weights\n\u001b[32m     77\u001b[39m             gradients = tape.gradient(loss, trainable_weights)\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m             \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m             self.optimizer.apply_gradients(zip(gradients, trainable_weights))\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m             warnings.warn(\u001b[33m\"The model does not have any trainable weights.\"\u001b[39m)\n\u001b[32m     83\u001b[39m \n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_gradients(self, grads_and_vars):\n\u001b[32m    382\u001b[39m         grads, trainable_variables = zip(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         self.apply(grads, trainable_variables)\n\u001b[32m    384\u001b[39m         \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._iterations\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    444\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m                 grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;28;01min\u001b[39;00m grads]\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m             \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m             self._backend_apply_gradients(grads, trainable_variables)\n\u001b[32m    449\u001b[39m             \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    450\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;28;01min\u001b[39;00m trainable_variables:\n\u001b[32m    451\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m variable.constraint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    507\u001b[39m             grads = self._clip_gradients(grads)\n\u001b[32m    508\u001b[39m             self._apply_weight_decay(trainable_variables)\n\u001b[32m    509\u001b[39m \n\u001b[32m    510\u001b[39m             \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m             self._backend_update_step(\n\u001b[32m    512\u001b[39m                 grads, trainable_variables, self.learning_rate\n\u001b[32m    513\u001b[39m             )\n\u001b[32m    514\u001b[39m \n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m trainable_variables\n\u001b[32m    117\u001b[39m         ]\n\u001b[32m    118\u001b[39m         grads_and_vars = list(zip(grads, trainable_variables))\n\u001b[32m    119\u001b[39m         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[32m    121\u001b[39m             self._distributed_tf_update_step,\n\u001b[32m    122\u001b[39m             self._distribution_strategy,\n\u001b[32m    123\u001b[39m             grads_and_vars,\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m   Returns:\n\u001b[32m     48\u001b[39m     The \u001b[38;5;28;01mreturn\u001b[39;00m value of the `fn` call.\n\u001b[32m     49\u001b[39m   \"\"\"\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, *args, **kwargs)\n\u001b[32m     52\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     return distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;28;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m             distribution.extended.update(\n\u001b[32m    135\u001b[39m                 var,\n\u001b[32m    136\u001b[39m                 apply_grad_to_update_var,\n\u001b[32m    137\u001b[39m                 args=(grad, learning_rate),\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m   3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   2882\u001b[39m \n\u001b[32m   2883\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m merge_fn(_, *merged_args, **merged_kwargs):\n\u001b[32m   2884\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self.update(var, fn, merged_args, merged_kwargs, group=group)\n\u001b[32m   2885\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m2886\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m replica_context.merge_call(merge_fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, merge_fn, args, kwargs)\u001b[39m\n\u001b[32m   3474\u001b[39m       kwargs = {}\n\u001b[32m   3475\u001b[39m \n\u001b[32m   3476\u001b[39m     merge_fn = autograph.tf_convert(\n\u001b[32m   3477\u001b[39m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._merge_call(merge_fn, args, kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, merge_fn, args, kwargs)\u001b[39m\n\u001b[32m   3483\u001b[39m         _CrossReplicaThreadMode(self._strategy))  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m   3484\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3485\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m merge_fn(self._strategy, *args, **kwargs)\n\u001b[32m   3486\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3487\u001b[39m       _pop_per_thread_mode()\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_, *merged_args, **merged_kwargs)\u001b[39m\n\u001b[32m   2883\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m merge_fn(_, *merged_args, **merged_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m2884\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self.update(var, fn, merged_args, merged_kwargs, group=group)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3001\u001b[39m         _get_default_replica_context()):\n\u001b[32m   3002\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update(self, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m     \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m     \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m       result = fn(*args, **kwargs)\n\u001b[32m   4082\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   4084\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/optimizers/adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m update_step(self, gradient, variable, learning_rate):\n\u001b[32m    116\u001b[39m         \u001b[33m\"\"\"Update step given gradient and the associated model variable.\"\"\"\u001b[39m\n\u001b[32m    117\u001b[39m         lr = ops.cast(learning_rate, variable.dtype)\n\u001b[32m    118\u001b[39m         gradient = ops.cast(gradient, variable.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         local_step = ops.cast(self.iterations + \u001b[32m1\u001b[39m, variable.dtype)\n\u001b[32m    120\u001b[39m         beta_1_power = ops.power(\n\u001b[32m    121\u001b[39m             ops.cast(self.beta_1, variable.dtype), local_step\n\u001b[32m    122\u001b[39m         )\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/common/variables.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __add__(self, other):\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m backend.numpy.add(self.value, other)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    489\u001b[39m                     x1 = tf.convert_to_tensor(x1)\n\u001b[32m    490\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m isinstance(x2, tf.IndexedSlices):\n\u001b[32m    491\u001b[39m                 \u001b[38;5;66;03m# x2 is an IndexedSlices, densify.\u001b[39;00m\n\u001b[32m    492\u001b[39m                 x2 = tf.convert_to_tensor(x2)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(x1, x2)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(x2.shape) > \u001b[32m1\u001b[39m:\n\u001b[32m    128\u001b[39m             x2 = tf.squeeze(x2)\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.nn.bias_add(x1, x2, data_format=data_format)\n\u001b[32m    130\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.add(x1, x2)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m   3909\u001b[39m     y = ops.convert_to_tensor(y, dtype_hint=x.dtype.base_dtype, name=\u001b[33m\"y\"\u001b[39m)\n\u001b[32m   3910\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.dtype == dtypes.string:\n\u001b[32m   3911\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.add(x, y, name=name)\n\u001b[32m   3912\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3913\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.add_v2(x, y, name=name)\n",
      "\u001b[32m~/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    478\u001b[39m         _ctx, \u001b[33m\"AddV2\"\u001b[39m, name, x, y)\n\u001b[32m    479\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    481\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m    483\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    485\u001b[39m       return add_v2_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=1000,\n",
    "                    batch_size=4096\n",
    "                    )\n",
    "\n",
    "# Optionally, print training history.\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_gumbel_softmax16noLN.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/x_val_robust_32const_ptetaphi.npy\")\n",
    "y_val = np.load(\"/Users/anrunw/Downloads/l1-jet-id/data/jetid/processed/y_val_robust_32const_ptetaphi.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260000"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout = np.argmax(model(x_val, training=False),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout = np.argmax(y_val,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7893807692307693"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(xout,yout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_45\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_45\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_169 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_26  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_27  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_178 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_179 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_45 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_169 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_26  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_27  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_178 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_179 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,297</span> (67.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,297\u001b[0m (67.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,765</span> (22.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,765\u001b[0m (22.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,532</span> (45.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m11,532\u001b[0m (45.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_45\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_45\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_169 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_26  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_27  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LinformerTransformerBlock</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_178 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_179 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_45 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_169 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_26  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ linformer_transformer_block_27  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mLinformerTransformerBlock\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_178 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_179 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,297</span> (67.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,297\u001b[0m (67.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,765</span> (22.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,765\u001b[0m (22.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,532</span> (45.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m11,532\u001b[0m (45.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anrunw/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/profiler/internal/flops_registry.py:471: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 00:39:31.465244: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2025-03-27 00:39:31.465348: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2025-03-27 00:39:31.465659: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-27 00:39:31.465676: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for a single inference:\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "BatchMatMulV2            363.52k float_ops (100.00%, 74.47%)\n",
      "Softmax                  81.94k float_ops (25.53%, 16.79%)\n",
      "RealDiv                  16.38k float_ops (8.74%, 3.36%)\n",
      "AddV2                    6.27k float_ops (5.39%, 1.28%)\n",
      "Mul                      6.14k float_ops (4.10%, 1.26%)\n",
      "BiasAdd                  4.63k float_ops (2.84%, 0.95%)\n",
      "Mean                     4.10k float_ops (1.89%, 0.84%)\n",
      "SquaredDifference        4.10k float_ops (1.06%, 0.84%)\n",
      "MatMul                     672 float_ops (0.22%, 0.14%)\n",
      "Rsqrt                      256 float_ops (0.08%, 0.05%)\n",
      "Neg                        128 float_ops (0.03%, 0.03%)\n",
      "\n",
      "======================End of Report==========================\n",
      " 488142\n",
      "Average inference time per event: 9240.842 nanoseconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def get_flops(model, input_shape):\n",
    "    # Create a concrete function from the Keras model.\n",
    "    input_tensor = tf.TensorSpec(input_shape, tf.float32)\n",
    "    concrete_func = tf.function(model).get_concrete_function(input_tensor)\n",
    "    \n",
    "    # Convert the model to a frozen graph.\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    \n",
    "    # Import the graph_def into a new graph.\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "        return flops.total_float_ops\n",
    "\n",
    "# Assuming x_val is already loaded and has shape (num_samples, features, ...)\n",
    "# For a single event, we use a batch size of 1.\n",
    "input_shape = [1] + list(x_val.shape[1:])\n",
    "total_flops = get_flops(model, input_shape)\n",
    "print(\"Total FLOPs for a single inference:\", total_flops)\n",
    "\n",
    "# ---------------------------\n",
    "# Time Inference Per Event\n",
    "# ---------------------------\n",
    "# Warm-up: Run one inference to load any lazy initializations.\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Warm-up (optional but recommended)\n",
    "_ = model(x_val[:10000])\n",
    "\n",
    "num_trials = 100\n",
    "inference_times = []\n",
    "for _ in range(num_trials):\n",
    "    start_time = time.perf_counter()\n",
    "    _ = model(x_val[:10000])\n",
    "    end_time = time.perf_counter()\n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "# Compute average inference time per event in seconds, then convert to nanoseconds\n",
    "avg_inference_time_seconds = np.mean(np.array(inference_times) / 10000)\n",
    "avg_inference_time_nanoseconds = avg_inference_time_seconds * 1e9\n",
    "\n",
    "print(\"Average inference time per event:\", np.round(avg_inference_time_nanoseconds,3), \"nanoseconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/FPR at TPR=80% for each class:\n",
      "g: 1/FPR = 9.729846269216347\n",
      "q: 1/FPR = 8.632307502162897\n",
      "W: 1/FPR = 37.00178094390027\n",
      "Z: 1/FPR = 51.57735286814005\n",
      "t: 1/FPR = 27.394663848897107\n",
      "Average 1/FPR across classes: 26.867190286463334\n",
      "\n",
      "Accuracy at the threshold corresponding to TPR=80% for each class:\n",
      "g: accuracy = 0.8776\n",
      "q: accuracy = 0.8678\n",
      "W: accuracy = 0.9382\n",
      "Z: accuracy = 0.9443\n",
      "t: accuracy = 0.9304\n",
      "Average accuracy across classes: 0.9117\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your class labels.\n",
    "class_labels = ['g', 'q', 'W', 'Z', 't']\n",
    "\n",
    "# Load your validation data.\n",
    "# (Assuming x_val and y_val are already loaded in your environment)\n",
    "# Get the model predictions as probabilities.\n",
    "# If your model outputs logits, you may need to apply softmax first.\n",
    "y_pred = model(x_val, training=False)\n",
    "\n",
    "# Convert predictions and labels from TensorFlow tensors to NumPy arrays if needed.\n",
    "if hasattr(y_pred, \"numpy\"):\n",
    "    y_pred = y_pred.numpy()\n",
    "if hasattr(y_val, \"numpy\"):\n",
    "    y_val = y_val.numpy()\n",
    "\n",
    "# Initialize dictionaries to store false positive rates, true positive rates, AUC, thresholds,\n",
    "# interpolated FPR at target TPR, and accuracy at that threshold.\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "thresholds_dict = {}\n",
    "fpr_at_target = {}\n",
    "accuracy_at_target = {}\n",
    "\n",
    "# Target true positive rate.\n",
    "target_tpr = 0.8\n",
    "\n",
    "# Compute ROC curve, AUC, and interpolate FPR and threshold at target TPR for each class.\n",
    "for i, label in enumerate(class_labels):\n",
    "    # Compute ROC curve: returns fpr, tpr, and thresholds.\n",
    "    fpr[i], tpr[i], thresholds = roc_curve(y_val[:, i], y_pred[:, i])\n",
    "    thresholds_dict[i] = thresholds  # save thresholds for later use\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Check if the target TPR is reached.\n",
    "    if target_tpr <= tpr[i][-1]:\n",
    "        # Interpolate FPR at target TPR.\n",
    "        fpr_at_target[label] = np.interp(target_tpr, tpr[i], fpr[i])\n",
    "        # Interpolate threshold at target TPR.\n",
    "        threshold_at_target = np.interp(target_tpr, tpr[i], thresholds)\n",
    "        # Compute binary predictions for the current class using the interpolated threshold.\n",
    "        preds_binary = (y_pred[:, i] >= threshold_at_target).astype(int)\n",
    "        # Compute accuracy for the current class.\n",
    "        accuracy = np.mean(preds_binary == y_val[:, i])\n",
    "        accuracy_at_target[label] = accuracy\n",
    "    else:\n",
    "        fpr_at_target[label] = None  # If target TPR is not reached.\n",
    "        accuracy_at_target[label] = None\n",
    "\n",
    "# Display the 1/FPR values when TPR=80% for each class.\n",
    "print(\"1/FPR at TPR=80% for each class:\")\n",
    "one_over_fpr = []\n",
    "for label in class_labels:\n",
    "    if fpr_at_target[label] is not None and fpr_at_target[label] != 0:\n",
    "        inv_val = 1.0 / fpr_at_target[label]\n",
    "        one_over_fpr.append(inv_val)\n",
    "        print(f\"{label}: 1/FPR = {inv_val}\")\n",
    "    else:\n",
    "        print(f\"{label}: undefined (FPR is None or zero)\")\n",
    "\n",
    "# Compute and print the average of 1/FPR across classes.\n",
    "if one_over_fpr:\n",
    "    avg_one_over_fpr = np.mean(one_over_fpr)\n",
    "    print(f\"Average 1/FPR across classes: {avg_one_over_fpr}\")\n",
    "else:\n",
    "    print(\"No valid 1/FPR values to compute an average.\")\n",
    "\n",
    "# Print the accuracy at the FPR corresponding to TPR=80% for each class.\n",
    "print(\"\\nAccuracy at the threshold corresponding to TPR=80% for each class:\")\n",
    "accuracies = []\n",
    "for label in class_labels:\n",
    "    if accuracy_at_target[label] is not None:\n",
    "        accuracies.append(accuracy_at_target[label])\n",
    "        print(f\"{label}: accuracy = {accuracy_at_target[label]:.4f}\")\n",
    "    else:\n",
    "        print(f\"{label}: accuracy undefined (target TPR not reached)\")\n",
    "if accuracies:\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    print(f\"Average accuracy across classes: {avg_accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"No valid accuracy values to compute an average.\")\n",
    "\n",
    "# Plot the ROC curves and mark the point corresponding to TPR=0.8.\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, label in enumerate(class_labels):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'ROC for class {label} (AUC = {roc_auc[i]:.2f})')\n",
    "    if fpr_at_target[label] is not None:\n",
    "        # Mark the point on the ROC curve where TPR = 0.8.\n",
    "        plt.plot(fpr_at_target[label], target_tpr, 'o', label=f'{label} @ TPR=0.8')\n",
    "\n",
    "# Plot the random chance line.\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Argmax Transformer - Average 1/FPR = ' + str(avg_one_over_fpr))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_124\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_124\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_557 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_83            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_46            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_563 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_564 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_135 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_557 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_83            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,128\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_46            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_563 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_564 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549</span> (9.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,549\u001b[0m (9.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549</span> (9.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,549\u001b[0m (9.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# ---------------------------\n",
    "# Gumbel Softmax Layer with Hard Sampling\n",
    "# ---------------------------\n",
    "class GumbelSoftmax(layers.Layer):\n",
    "    \"\"\"\n",
    "    Applies the Gumbel Softmax trick to sample from a categorical distribution\n",
    "    in a differentiable way. If hard=True, uses the straight-through estimator\n",
    "    to return one-hot vectors in the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=1.0, hard=False, **kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = tf.random.uniform(shape, minval=0, maxval=1)\n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def call(self, logits):\n",
    "        noise = self.sample_gumbel(tf.shape(logits))\n",
    "        y = logits + noise\n",
    "        y_soft = tf.nn.softmax(y / self.temperature, axis=-1)\n",
    "        if self.hard:\n",
    "            # Straight-through estimator: one-hot encode the sample.\n",
    "            y_hard = tf.one_hot(tf.argmax(y_soft, axis=-1), depth=tf.shape(y_soft)[-1])\n",
    "            y = tf.stop_gradient(y_hard - y_soft) + y_soft\n",
    "            return y\n",
    "        else:\n",
    "            return y_soft\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregation Layer\n",
    "# ---------------------------\n",
    "class AggregationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Aggregates a set of features over the sequence dimension.\n",
    "    Supported aggregations: \"mean\" or \"max\".\n",
    "    \"\"\"\n",
    "    def __init__(self, aggreg=\"mean\", **kwargs):\n",
    "        super(AggregationLayer, self).__init__(**kwargs)\n",
    "        self.aggreg = aggreg\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.aggreg == \"mean\":\n",
    "            return tf.reduce_mean(inputs, axis=1)\n",
    "        elif self.aggreg == \"max\":\n",
    "            return tf.reduce_max(inputs, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Given aggregation string is not implemented. Use 'mean' or 'max'.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Custom Multi-Head Attention with Gumbel Softmax\n",
    "# ---------------------------\n",
    "class CustomMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, attn_temperature=1.0, **kwargs):\n",
    "        super(CustomMultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.attn_temperature = attn_temperature\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Define weight matrices for Q, K, V.\n",
    "        self.wq = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wq\")\n",
    "        self.wk = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wk\")\n",
    "        self.wv = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wv\")\n",
    "        self.dense = layers.Dense(self.d_model)\n",
    "        super(CustomMultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Reshape x to (batch_size, seq_len, num_heads, depth) then transpose.\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Compute Q, K, V.\n",
    "        q = tf.matmul(x, self.wq)  # (batch_size, seq_len, d_model)\n",
    "        k = tf.matmul(x, self.wk)\n",
    "        v = tf.matmul(x, self.wv)\n",
    "\n",
    "        # Split into multiple heads.\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Compute scaled dot-product attention logits.\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Replace Gumbel softmax with argmax and one-hot encoding.\n",
    "        indices = tf.argmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = tf.one_hot(indices, depth=tf.shape(scaled_attention_logits)[-1], dtype=scaled_attention_logits.dtype)\n",
    "\n",
    "\n",
    "        # Compute attention output.\n",
    "        scaled_attention = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Concatenate heads.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Final linear layer.\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Block\n",
    "# ---------------------------\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, output_dim, num_heads, attn_temperature=1.0, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.mha = CustomMultiHeadAttention(d_model, num_heads, attn_temperature=attn_temperature)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.outD = layers.Dense(16)\n",
    "        self.outD2 = layers.Dense(16)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Multi-head attention sub-layer.\n",
    "        attn_output = self.mha(x)\n",
    "        out1 = self.outD(x + attn_output)\n",
    "        # Feed-forward sub-layer.\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = out1 + ffn_output\n",
    "        out2 = self.outD2(out2)\n",
    "        return out2\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definition\n",
    "# ---------------------------\n",
    "def build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                          d_model=16, d_ff=16, output_dim=16,\n",
    "                                          num_heads=1, attn_temperature=1.0):\n",
    "    \"\"\"\n",
    "    Builds a classifier model with:\n",
    "      - A linear embedding layer.\n",
    "      - Multiple transformer encoder blocks using Gumbel Softmax in attention.\n",
    "      - Aggregation over the sequence dimension using AggregationLayer.\n",
    "      - A final linear output layer for 5 classes.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_particles, feature_dim))\n",
    "    \n",
    "    # Linear embedding.\n",
    "    x = layers.Dense(16, activation='relu')(inputs)\n",
    "    \n",
    "    # Apply multiple transformer blocks.\n",
    "    x = TransformerBlock(16, 16, output_dim=8, num_heads=8, attn_temperature=attn_temperature)(x)\n",
    "    \n",
    "    # Custom aggregation (using \"max\" aggregation here).\n",
    "    pooled_output = AggregationLayer(aggreg='max')(x)\n",
    "    \n",
    "    # Final dense layers.\n",
    "    x = layers.Dense(16, activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "import numpy as np\n",
    "\n",
    "# Load your data.\n",
    "# Determine input dimensions.\n",
    "num_particles = x_train.shape[1]\n",
    "feature_dim = x_train.shape[-1]\n",
    "\n",
    "# Build and compile the model.\n",
    "model = build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                            d_model=8, d_ff=8,\n",
    "                                            output_dim=8, num_heads=8,\n",
    "                                            attn_temperature=0.1)\n",
    "model.load_weights('model_gumbel_softmax16noLN.weights.h5')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Print model summary.\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Optionally, print training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU(s):\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Weights loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_102\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_102\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_463 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_74            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_35            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AggregationLayer</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_469 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_470 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_112 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_463 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_74            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,128\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aggregation_layer_35            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAggregationLayer\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_469 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_470 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549</span> (9.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,549\u001b[0m (9.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549</span> (9.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,549\u001b[0m (9.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anrunw/miniconda3/envs/tf_env/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check for available GPUs.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Using GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs found. Ensure tensorflow-macos and tensorflow-metal are installed for GPU acceleration on M2 Macs.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Gumbel Softmax Layer with Hard Sampling\n",
    "# ---------------------------\n",
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, hard=False, **kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = tf.random.uniform(shape, minval=0, maxval=1)\n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def call(self, logits):\n",
    "        noise = self.sample_gumbel(tf.shape(logits))\n",
    "        y = logits + noise\n",
    "        y_soft = tf.nn.softmax(y / self.temperature, axis=-1)\n",
    "        if self.hard:\n",
    "            y_hard = tf.one_hot(tf.argmax(y_soft, axis=-1), depth=tf.shape(y_soft)[-1])\n",
    "            # Use the straight-through estimator.\n",
    "            y = tf.stop_gradient(y_hard - y_soft) + y_soft\n",
    "            return y\n",
    "        else:\n",
    "            return y_soft\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregation Layer\n",
    "# ---------------------------\n",
    "class AggregationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Aggregates a set of features over the sequence dimension.\n",
    "    Supported aggregations: \"mean\" or \"max\".\n",
    "    \"\"\"\n",
    "    def __init__(self, aggreg=\"mean\", **kwargs):\n",
    "        super(AggregationLayer, self).__init__(**kwargs)\n",
    "        self.aggreg = aggreg\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.aggreg == \"mean\":\n",
    "            return tf.reduce_mean(inputs, axis=1)\n",
    "        elif self.aggreg == \"max\":\n",
    "            return tf.reduce_max(inputs, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Aggregation not implemented. Use 'mean' or 'max'.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Custom Multi-Head Attention with Gumbel Softmax\n",
    "# ---------------------------\n",
    "class CustomMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, attn_temperature=1.0, **kwargs):\n",
    "        super(CustomMultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.attn_temperature = attn_temperature\n",
    "        self.attn_matrix = []  # List to store attention matrices.\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Define weight matrices for Q, K, V.\n",
    "        self.wq = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wq\")\n",
    "        self.wk = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wk\")\n",
    "        self.wv = self.add_weight(shape=(self.d_model, self.d_model),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True,\n",
    "                                  name=\"wv\")\n",
    "        self.dense = layers.Dense(self.d_model)\n",
    "        super(CustomMultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Reshape x to (batch_size, seq_len, num_heads, depth) then transpose.\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def reset_attention_matrix(self):\n",
    "        \"\"\"Clears the stored attention matrices.\"\"\"\n",
    "        self.attn_matrix = []\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Compute Q, K, V.\n",
    "        q = tf.matmul(x, self.wq)  # (batch_size, seq_len, d_model)\n",
    "        k = tf.matmul(x, self.wk)\n",
    "        v = tf.matmul(x, self.wv)\n",
    "\n",
    "        # Split into multiple heads.\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Compute scaled dot-product attention logits.\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply Gumbel Softmax (with hard sampling) instead of standard softmax.\n",
    "        gumbel_layer = GumbelSoftmax(temperature=self.attn_temperature, hard=True)\n",
    "        attention_weights = gumbel_layer(scaled_attention_logits)\n",
    "\n",
    "        # Compute attention output.\n",
    "        scaled_attention = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Concatenate heads.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Final linear layer.\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        # Use tf.py_function to convert attention_weights to a concrete (numpy-backed) tensor.\n",
    "        # The lambda here receives a numpy array and returns it.\n",
    "        np_attention = tf.py_function(func=lambda t: t, inp=[attention_weights], Tout=tf.float32)\n",
    "        np_attention.set_shape(attention_weights.shape)\n",
    "        self.attn_matrix.append(np_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Block\n",
    "# ---------------------------\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, output_dim, num_heads, attn_temperature=1.0, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.mha = CustomMultiHeadAttention(d_model, num_heads, attn_temperature=attn_temperature)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.outD = layers.Dense(16)\n",
    "        self.outD2 = layers.Dense(16)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Multi-head attention sub-layer.\n",
    "        attn_output, _ = self.mha(x)\n",
    "        out1 = self.outD(x + attn_output)\n",
    "        # Feed-forward sub-layer.\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = out1 + ffn_output\n",
    "        out2 = self.outD2(out2)\n",
    "        return out2\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definition\n",
    "# ---------------------------\n",
    "def build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                          d_model=16, d_ff=16, output_dim=16,\n",
    "                                          num_heads=1, attn_temperature=0.5):\n",
    "    \"\"\"\n",
    "    Builds a classifier model with:\n",
    "      - A linear embedding layer.\n",
    "      - A transformer encoder block using Gumbel Softmax in attention.\n",
    "      - Aggregation over the sequence dimension using AggregationLayer.\n",
    "      - A final dense output layer for 5 classes.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(num_particles, feature_dim))\n",
    "    \n",
    "    # Linear embedding.\n",
    "    x = layers.Dense(16, activation='relu')(inputs)\n",
    "\n",
    "    # Apply a transformer block.\n",
    "    transformer_block = TransformerBlock(16, 16, output_dim=8, num_heads=num_heads, attn_temperature=attn_temperature)\n",
    "    x = transformer_block(x)\n",
    "    \n",
    "    # Custom aggregation (using \"max\" aggregation here).\n",
    "    pooled_output = AggregationLayer(aggreg='max')(x)\n",
    "    \n",
    "    # Final dense layers.\n",
    "    x = layers.Dense(16, activation='relu')(pooled_output)\n",
    "    outputs = layers.Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # Attach the transformer block to the model for later access.\n",
    "    model.transformer_block = transformer_block\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plotting Function for Attention Matrices\n",
    "# ---------------------------\n",
    "def plot_attention_matrices(attn_matrix, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Expects attn_matrix to be a list of concrete (numpy) tensors, where each element has shape:\n",
    "      (batch_size, num_heads, seq_len, seq_len)\n",
    "    This function plots the attention map for the first sample in the batch\n",
    "    across all heads from the last recorded attention weights.\n",
    "    \"\"\"\n",
    "    if not attn_matrix:\n",
    "        print(\"No attention matrices to plot.\")\n",
    "        return\n",
    "\n",
    "    # Evaluate the last stored attention matrix (if needed).\n",
    "    weights = attn_matrix[-1]\n",
    "    # If weights are still a tf.Tensor, convert to numpy.\n",
    "    if hasattr(weights, \"numpy\"):\n",
    "        weights = weights.numpy()\n",
    "        \n",
    "    # Check that we have a batch dimension.\n",
    "    if weights.ndim != 4:\n",
    "        print(\"Unexpected shape for attention weights:\", weights.shape)\n",
    "        return\n",
    "\n",
    "    batch_size, num_heads, seq_len, _ = weights.shape\n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(num_heads * 4, 4))\n",
    "    if num_heads == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(num_heads):\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(weights[0, i, :, :], cmap='viridis')\n",
    "        ax.set_title(f'Head {i + 1}')\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training / Evaluation / Plotting Example\n",
    "# ---------------------------\n",
    "# For demonstration, we'll create dummy data.\n",
    "num_particles = 16\n",
    "feature_dim = 3  # (Make sure this matches your data)\n",
    "\n",
    "# Dummy data.\n",
    "\n",
    "# Build the model.\n",
    "model = build_custom_transformer_classifier(num_particles, feature_dim,\n",
    "                                            d_model=8, d_ff=8,\n",
    "                                            output_dim=8, num_heads=4,\n",
    "                                            attn_temperature=0.1)\n",
    "\n",
    "# Attempt to load saved weights.\n",
    "try:\n",
    "    model.load_weights('model_gumbel_softmax16noLN.weights.h5')\n",
    "    print(\"Weights loaded successfully.\")\n",
    "except ValueError as e:\n",
    "    print(\"Error loading weights:\", e)\n",
    "    print(\"Skipping weight loading. Ensure the model architecture matches the saved weights if you intend to load them.\")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# (Optional) Train the model.\n",
    "# model.fit(x_train, y_train, epochs=5, batch_size=16)\n",
    "\n",
    "# Clear any stored attention matrices.\n",
    "model.transformer_block.mha.reset_attention_matrix()\n",
    "\n",
    "# Run a prediction to record attention weights.\n",
    "sample_input = x_train[:5]  # Use a few samples.\n",
    "predictions = model.predict(sample_input)\n",
    "\n",
    "# Retrieve and plot the attention weights from the custom multi-head attention layer.\n",
    "attn_matrix = model.transformer_block.mha.attn_matrix\n",
    "plot_attention_matrices(attn_matrix, title=\"Attention Weights from Sample Prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16, 3)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
